{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f5179-a34e-432c-87e0-175c2155e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime, time\n",
    "import pickle, os, vitaldb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_match = pd.read_csv('ftn+_age_match_20201224-Copy1.csv',sep=',')\n",
    "df_match['NRS_time']=pd.to_datetime(df_match['NRS_time'])\n",
    "df_match = df_match.rename(columns={'Unnamed: 0':'index'})\n",
    "#df_match.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(len(df_match))\n",
    "'''vital -> csv\n",
    "df_match['path']=[f_name.split('.')[0]+\".csv\" for f_name in df_match['path'].values.flatten()] '''\n",
    "\n",
    "\n",
    "error_list=[]\n",
    "srate= 250\n",
    "for i in range(len(df_match)):\n",
    "    filename = str(df_match.loc[i,'Value'])+','+str(df_match.loc[i,'index'])+','+ df_match.loc[i,'path']\n",
    "    vital_path = '../../../cranberry2/Preprocessing/vital_data/ECG_250Hz_pacu_5min/'+filename\n",
    "    \n",
    "    if os.path.exists(vital_path):\n",
    "    #if not os.path.exists('../../../cranberry2/Preprocessing/vital_data/ECG_250Hz_pacu_5min/'+filename):\n",
    "        print(i, end=' ')\n",
    "\n",
    "        rec_path = '../../../cranberry2/Preprocessing/vital_data/pacu_full_vital/'+df_match.loc[i,'path']\n",
    "        \n",
    "        \n",
    "        # vitaldb 에러에 해당되는 케이스\n",
    "        df_vital = pickle.load(open(vital_path,'rb'))\n",
    "        \n",
    "        if np.sum(~np.isnan(df_vital['ECG'].tolist())) != 0:\n",
    "            if np.nanmean(df_vital['ECG'].tolist()) > -4:\n",
    "                print('already done')\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        error_list.append(filename)\n",
    "        print(df_match.loc[i,'path'], end='... ')\n",
    "        \n",
    "        vfile_ecg = vitaldb.vital_recs_time(rec_path,['SNUADCW/ECG_II'],interval=0.004).flatten()\n",
    "\n",
    "        \n",
    "        if len(vfile_ecg[0])==0 :\n",
    "            print('index ',df_match.loc[i,'index'],' OPID ',df_match.loc[i,'opid'],'empty vital file')\n",
    "            \n",
    "        else:\n",
    "            Abs_time = [vfile_ecg[1]+datetime.timedelta(hours=9)+datetime.timedelta(seconds=i/srate) for i in range(len(vfile_ecg[0]))]\n",
    "            #print(vfile[1])\n",
    "            dics = {'Abs_time':Abs_time,'ECG':vfile_ecg[0]}\n",
    "            df_vital = pd.DataFrame(data=dics)\n",
    "            end_idx_list = df_vital[(df_vital['Abs_time'] > df_match.loc[i,'NRS_time'] - datetime.timedelta(seconds=1)) & (df_vital['Abs_time'] < df_match.loc[i,'NRS_time'] + datetime.timedelta(seconds=1))].index.tolist()\n",
    "\n",
    "            \n",
    "            if len(end_idx_list)==0:\n",
    "                print('index ',i,' OPID ',df_match.loc[i,'opid'],\" no vital data at NRS time\")\n",
    "\n",
    "            else:\n",
    "                end_idx=end_idx_list[0]+1\n",
    "                start_idx= end_idx - 5 * 60 * srate\n",
    "                start_idx_n = max(0,start_idx)\n",
    "\n",
    "                ext = df_vital[start_idx_n:end_idx]\n",
    "                extr=ext.dropna(subset=['ECG'])\n",
    "                extr=extr.drop_duplicates(subset=['ECG'])\n",
    "\n",
    "                if len(extr)>1:\n",
    "                    if start_idx <0 :\n",
    "                        fit = (-1)*end_idx \n",
    "                        extraction = pd.DataFrame(index=range(5*60*srate),columns=['ECG'])\n",
    "                        \n",
    "\n",
    "                        col_ecg = np.full(5*60*srate, np.nan)\n",
    "                        col_ecg[fit:] = ext['ECG'][fit:].tolist()                    \n",
    "                        extraction['ECG'] = col_ecg                        \n",
    "                        \n",
    "\n",
    "                    else:\n",
    "                        extraction = ext\n",
    "                        \n",
    "                    with open('../../../cranberry2/Preprocessing/vital_data/ECG_250Hz_pacu_5min/'+filename,'wb') as f:\n",
    "                        pickle.dump(extraction, f)\n",
    "                    print('...done')\n",
    "                else:\n",
    "                    print('...결측치/직선 데이터')\n",
    "                \n",
    "        #except:\n",
    "            #print('error: ',i,df_match.loc[i,'opid'],df_match.loc[i,'path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c95c4-5d8d-4f29-b912-f851c7271795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import vitaldb\n",
    "from pyvital.pyvital import arr\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import time, datetime\n",
    "import neurokit2 as nk\n",
    "from tqdm import tqdm\n",
    "\n",
    "SRATE = 300\n",
    "LEN_INPUT = 30\n",
    "STRIDE = 10\n",
    "LEN_PER_PRE = 30\n",
    "LEN_PER_POST = 30\n",
    "\n",
    "\n",
    "# 피크 사이 wave를 모두 같은 length로 만들기 위한 함수\n",
    "def linear_connection(list, idx):\n",
    "    int_idx = int(idx)\n",
    "    return list[int_idx] + (list[int_idx+1] - list[int_idx]) * (idx - int_idx)\n",
    "\n",
    "def lowess(y, f=0.2):\n",
    "    x = np.arange(0, len(y))\n",
    "    return sm.nonparametric.lowess(y, x, frac=f, it=0)[:, 1].T\n",
    "## 60초짜리 inputp에 대해 f 그려보기\n",
    "\n",
    "\n",
    "def ECG_filter(seg, method, srate):\n",
    "    if method == '3rd-bandpass':\n",
    "        import scipy.signal\n",
    "        sos = scipy.signal.butter(3, [1,47], 'bandpass', output='sos', fs=srate)\n",
    "        return scipy.signal.sosfilt(sos, seg)\n",
    "    # lowess랑 수학적으로 동일한데 더 빠름 - 대신 계수 체크 필요\n",
    "    elif method == 'savgol':\n",
    "        import scipy.signal\n",
    "        return seg - scipy.signal.savgol_filter(seg, 1151, 3)\n",
    "    \n",
    "    elif method == 'lowess':\n",
    "        import statsmodels.api as sm\n",
    "        x = np.arange(0, len(seg))\n",
    "        return seg - sm.nonparametric.lowess(seg, x, frac=0.2, it=0)[:, 1].T\n",
    "\n",
    "    else:\n",
    "        print('not a valid filter')\n",
    "        return seg\n",
    "\n",
    "    \n",
    "def PPG_filter(seg, method, srate):\n",
    "    if method == 'bandpass':\n",
    "        import scipy.signal\n",
    "        sos = scipy.signal.butter(5, [1,40], 'bandpass', output='sos', fs=srate)\n",
    "        return scipy.signal.sosfilt(sos, seg)\n",
    "    \n",
    "    elif method == 'lowess':\n",
    "        import statsmodels.api as sm\n",
    "        x = np.arange(0, len(seg))\n",
    "        return seg - sm.nonparametric.lowess(seg, x, frac=0.2, it=0)[:, 1].T\n",
    "    \n",
    "    else:\n",
    "        print('not a valid filter')\n",
    "        return seg\n",
    "    \n",
    "    \n",
    "def quality_assessment(seg_ppg, seg_ecg, corr_thres):\n",
    "    # peak detection\n",
    "    try:\n",
    "        min_peak, ppg_peak = arr.detect_peaks(pd.DataFrame(seg_ppg).fillna(method='ffill', axis=0).fillna(method='bfill', axis=0).values.flatten(), SRATE)\n",
    "        signals, info = nk.ecg_peaks(pd.DataFrame(seg_ecg).fillna(method='ffill', axis=0).fillna(method='bfill', axis=0).values.flatten(), sampling_rate = SRATE)\n",
    "        ecg_peak = info[\"ECG_R_Peaks\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        #print('error of', e)\n",
    "        return False\n",
    "        \n",
    "    if len(ppg_peak)==0 or len(ecg_peak)==0:\n",
    "        #print('no peak')\n",
    "        return False\n",
    "\n",
    "\n",
    "    # segment 내의 ppg, ecg peak idx\n",
    "    idx_ppg_peak = ppg_peak\n",
    "    idx_ecg_peak = ecg_peak\n",
    "\n",
    "\n",
    "    # peak가 HR 30~150 -> 20s - min 10 peaks(HR30)\n",
    "    # peak 개수가 기준 미달이면 noise 계산 자세히 할 필요없이 False - 이 경우의 noise_info는 -2로 처리\n",
    "    if len(idx_ppg_peak)<5/10*LEN_INPUT or len(idx_ecg_peak)<5/10*LEN_INPUT:\n",
    "        #print(' too less peaks', end='')\n",
    "        return False\n",
    "    \n",
    "\n",
    "    # peak와 peak 사이 interval에 대한 noise 여부 -> 따라서 길이는 peak - 1 (noise면 True)\n",
    "    bool_noise_ppg = [False for k in range(len(idx_ppg_peak)-1)]\n",
    "    bool_noise_ecg = [False for k in range(len(idx_ecg_peak)-1)]\n",
    "\n",
    "\n",
    "    #  2.1 peak 간격 이상한 noise (HR 30~150 -> HBI 0.4s ~ 2s로 SRATE 곱해주면 40~200)\n",
    "    for k in range(len(bool_noise_ppg)):\n",
    "        if not 0.4*SRATE < idx_ppg_peak[k+1] - idx_ppg_peak[k] < 2*SRATE:\n",
    "            bool_noise_ppg[k] = True\n",
    "    for k in range(len(bool_noise_ecg)):\n",
    "        if not 0.4*SRATE < idx_ecg_peak[k+1] - idx_ecg_peak[k] < 2*SRATE:\n",
    "            bool_noise_ecg[k] = True\n",
    "\n",
    "\n",
    "    # 2.2 모양 이상한 noise\n",
    "    # wave interval into same length(2s(200))\n",
    "    len_wave = 2*SRATE\n",
    "    norm_seg_ppg, norm_seg_ecg = [], []\n",
    "\n",
    "    for k in range(len(bool_noise_ppg)):\n",
    "        len_interval_ppg = idx_ppg_peak[k+1] - idx_ppg_peak[k]\n",
    "        # peak 사이 wave를 모두 같은 길이로 변환\n",
    "        norm_seg_ppg.append([linear_connection(seg_ppg[idx_ppg_peak[k]:idx_ppg_peak[k+1]+1], n/len_wave*len_interval_ppg) for n in range(len_wave)])\n",
    "\n",
    "    for k in range(len(bool_noise_ecg)):\n",
    "        len_interval_ecg = idx_ecg_peak[k+1] - idx_ecg_peak[k]\n",
    "        # peak 사이 wave를 모두 같은 길이로 변환\n",
    "        norm_seg_ecg.append([linear_connection(seg_ecg[idx_ecg_peak[k]:idx_ecg_peak[k+1]+1], n/len_wave*len_interval_ecg) for n in range(len_wave)])\n",
    "\n",
    "    # wave interval 사이 correlation 계산 - PPG\n",
    "    corr_ppg = []\n",
    "    mean_wave_ppg = np.nanmean(norm_seg_ppg, axis = 0)\n",
    "    mean_wave_ppg = pd.DataFrame(mean_wave_ppg).fillna(method='ffill', axis=0).fillna(method='bfill', axis=0).values.flatten()\n",
    "    norm_seg_ppg = pd.DataFrame(norm_seg_ppg).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).values\n",
    "    for k in range(len(bool_noise_ppg)):\n",
    "        corr = np.corrcoef(norm_seg_ppg[k], mean_wave_ppg)[0,1]\n",
    "        corr_ppg.append(corr)\n",
    "    noise_ppg_perc = np.mean(bool_noise_ppg | (np.array(corr_ppg) < corr_thres))\n",
    "\n",
    "\n",
    "    # wave interval 사이 correlation 계산 - ECG   \n",
    "    corr_ecg = []\n",
    "    mean_wave_ecg = np.nanmean(norm_seg_ecg, axis = 0)\n",
    "    mean_wave_ecg = pd.DataFrame(mean_wave_ecg).fillna(method='ffill', axis=0).fillna(method='bfill', axis=0).values.flatten()\n",
    "    norm_seg_ecg = pd.DataFrame(norm_seg_ecg).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1).values\n",
    "    for k in range(len(bool_noise_ecg)):\n",
    "        corr = np.corrcoef(norm_seg_ecg[k], mean_wave_ecg)[0,1]\n",
    "        corr_ecg.append(corr)\n",
    "    noise_ecg_perc = np.mean(bool_noise_ecg | (np.array(corr_ecg) < corr_thres))                          \n",
    "\n",
    "    \n",
    "    # segment의 noise 비율 정보를 return\n",
    "    return [noise_ppg_perc, noise_ecg_perc]\n",
    "\n",
    "\n",
    "def preprocess(file_path, SRATE, ECG_FILT, PPG_FILT, CORR_THRES):\n",
    "    ### Preprocess of input : Denosing -> Quality Assessment\n",
    "    ### stores the result of preprocess in dataframe\n",
    "    # path for cache\n",
    "    if not os.path.exists('./cache'):\n",
    "        os.mkdir('./cache')     \n",
    "    if not os.path.exists('./cache/preprocess'):\n",
    "        os.mkdir('./cache/preprocess')\n",
    "\n",
    "    hyper_path = f'SRATE{SRATE}_LEN{LEN_INPUT}-PRE{LEN_PER_PRE}-POST{LEN_PER_POST}_STR{STRIDE}_PPG-{PPG_FILT}_ECG-{ECG_FILT}'\n",
    "    input_path = f\"../DL_model/dataset/PD_{hyper_path}/\"\n",
    "    if not os.path.exists('../DL_model/dataset'):\n",
    "        os.mkdir('../DL_model/dataset')\n",
    "    if not os.path.exists(input_path[:-1]):\n",
    "        os.mkdir(input_path[:-1])  \n",
    "        \n",
    "\n",
    "    # dataframe to save preprocessing info\n",
    "    n_aug = int((LEN_PER_PRE-LEN_INPUT)/STRIDE) + 1\n",
    "    column_list = ['caseid'] + [f'{i+1}' for i in np.arange(n_aug)] + ['age', 'gender']\n",
    "    df_preprocess = pd.DataFrame(columns = column_list)\n",
    "\n",
    "    # df_preprocess에 demographs(age, gender) 추가\n",
    "    df_demograph = pd.read_csv(\"https://api.vitaldb.net/cases\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # set variables\n",
    "    caseids = os.listdir(file_path)\n",
    "    f_num = 0\n",
    "    initial, interval = f_num, len(caseids)\n",
    "    \n",
    "    # variables\n",
    "    non_lis, low_mbp = [], []\n",
    "    x, x_rftn, y, c = [], [], [], []\n",
    "    age, gender, y_mbp = [], [], []\n",
    "    nan_hr, nan_mbp, nan_rftn = 0, 0, 0\n",
    "\n",
    "    start = time.time()\n",
    "    pbar = tqdm(caseids[initial:initial+interval])\n",
    "    for caseid in pbar:\n",
    "        caseid = caseid[:-4]   # '123.npz' -> '123'\n",
    "        f_num += 1\n",
    "\n",
    "        # vital data 불러오기\n",
    "        try:\n",
    "            vals = np.load(f'{file_path}/{caseid}.npz')\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # baseline 혈압이 낮은 경우(<65mmHg)는 제외: ephedrine, phenylephrine 투여 가능성\n",
    "        nmbp = pd.DataFrame(vals['nMBP']).fillna(method='ffill', axis=0).fillna(method='bfill', axis=0).values.flatten()\n",
    "        if len(nmbp) == 0:\n",
    "            nan_mbp = nan_mbp + 1\n",
    "            continue\n",
    "        if nmbp[-1] < 65:\n",
    "            low_mbp.append(caseid)\n",
    "            continue\n",
    "            \n",
    "\n",
    "        ### before intubation event (-120 ~ -60sec) preprocessing  \n",
    "        #n_aug = int((LEN_PER_PRE-LEN_INPUT)/STRIDE) + 1\n",
    "        segs_ppg = vals['nPPG'][-LEN_PER_PRE*SRATE:]\n",
    "        segs_ecg = vals['nECG'][-LEN_PER_PRE*SRATE:]\n",
    "        for i in range(n_aug):\n",
    "            # vital data\n",
    "            seg_ppg = segs_ppg[i*STRIDE*SRATE:i*STRIDE*SRATE+LEN_INPUT*SRATE]\n",
    "            seg_ecg = segs_ecg[i*STRIDE*SRATE:i*STRIDE*SRATE+LEN_INPUT*SRATE]\n",
    "            \n",
    "            if len(seg_ppg) == 0 or len(seg_ecg) == 0:\n",
    "                #print('not valid ecg, ppg', end='...')\n",
    "                continue\n",
    "\n",
    "            seg_ppg = [np.nan if ele <=-200 else ele for ele in seg_ppg]\n",
    "            seg_ecg = [np.nan if ele <=-4 else ele for ele in seg_ecg]\n",
    "\n",
    "            #dataframe에 새로운 행 만들기\n",
    "            df_preprocess.loc[f_num-1,'caseid'] = caseid\n",
    "            \n",
    "            row_demo = df_demograph[df_demograph['caseid']==int(caseid)]\n",
    "            df_preprocess.loc[f_num-1, 'age'] = row_demo['age'].values[0]\n",
    "            df_preprocess.loc[f_num-1, 'gender'] = row_demo['sex'].values[0]\n",
    "\n",
    "            \n",
    "            ## 1. 결측치 제거 ##\n",
    "            nan_ppg_perc = np.mean(np.isnan(seg_ppg))\n",
    "            nan_ecg_perc = np.mean(np.isnan(seg_ecg))\n",
    "\n",
    "            # segment의 결측치 비율 정보\n",
    "            nan_info = [nan_ppg_perc, nan_ecg_perc]\n",
    "\n",
    "            # 결측치가 많은 경우, noise 확인할 것도 없이 False -  이 경우의 noise_info는 -1로 처리\n",
    "            if nan_ppg_perc > 0.05 or nan_ecg_perc > 0.05:\n",
    "                #print(' too much missing data', end='...')\n",
    "                df_preprocess.loc[f_num-1,f'{i+1}'] = (False, nan_info, [-1, -1])\n",
    "                continue\n",
    "\n",
    "\n",
    "            ## 2. Denosing ##\n",
    "            # ECG : 1-40Hz bandpass filter / PPG : loess filter\n",
    "            seg_ecg = ECG_filter(pd.DataFrame(seg_ecg).fillna(method='ffill', axis=0).fillna(method='bfill', axis=0).values.flatten(), method=ECG_FILT, srate=SRATE)\n",
    "            seg_ppg = PPG_filter(pd.DataFrame(seg_ppg).fillna(method='ffill', axis=0).fillna(method='bfill', axis=0).values.flatten(), method=PPG_FILT, srate=SRATE)\n",
    "\n",
    "\n",
    "            ## 3. Quality Assessment ## \n",
    "            # segment의 noise 비율 정보\n",
    "            noise_info = quality_assessment(seg_ppg, seg_ecg, CORR_THRES) \n",
    "            \n",
    "            # Fail the preprocess\n",
    "            if not noise_info:\n",
    "                df_preprocess.loc[f_num-1,f'{i+1}'] = (False, nan_info, [-2, -2])\n",
    "                continue\n",
    "\n",
    "\n",
    "            # segment를 input으로 써도 되는지\n",
    "            bool_pass = [True if noise_info[0] < 0.1 and noise_info[1] < 0.1 else False]\n",
    "\n",
    "\n",
    "            # 이 segment의 정보를 dataframe에 저장 - (전처리 성공여부, 전처리 nan 비율, 전처리 noise 비율, 통증 점수)\n",
    "            df_preprocess.loc[f_num-1,f'{i+1}'] = (bool_pass[0], nan_info, noise_info)\n",
    "            #print(f'preprocessing done...{bool_pass[0]}', end='')\n",
    "            \n",
    "            \n",
    "            if bool_pass[0]:\n",
    "                rftn = vals['nRFTN'][-LEN_PER_POST*SRATE:][i*STRIDE*SRATE:i*STRIDE*SRATE+LEN_INPUT*SRATE]\n",
    "                rftn = pd.DataFrame(rftn).fillna(method='ffill', axis=0).fillna(method='bfill', axis=0).values.flatten()\n",
    "                rftn = rftn / 8\n",
    "                if np.mean(np.isnan(rftn)) != 0:\n",
    "                    nan_rftn = nan_rftn + 1\n",
    "                    continue\n",
    "\n",
    "                # z-score normalization\n",
    "                ppg_input = stats.zscore(seg_ppg)\n",
    "                ecg_input = stats.zscore(seg_ecg)\n",
    "\n",
    "                \n",
    "                hr = vals['HR'][:LEN_PER_POST*SRATE][i*STRIDE*SRATE:i*STRIDE*SRATE+LEN_INPUT*SRATE]\n",
    "                nhr = vals['nHR'][-LEN_PER_POST*SRATE:][i*STRIDE*SRATE:i*STRIDE*SRATE+LEN_INPUT*SRATE]\n",
    "                if np.nanmean(hr) == 0 or np.nanmean(nhr) == 0:\n",
    "                    #print(f'abnormal HR value of caseid {caseid}') \n",
    "                    nan_hr = nan_hr + 1\n",
    "                    continue\n",
    "                del_hr = np.nanmean(hr) / np.nanmean(nhr) - 1\n",
    "                \n",
    "                if np.isnan(del_hr):\n",
    "                    #print(f'  del_hr of caseid {caseid} value is nan')\n",
    "                    nan_hr = nan_hr + 1\n",
    "                    continue\n",
    "\n",
    "                # MBP\n",
    "                del_mbp = np.nanmean(vals['MBP'][:LEN_PER_POST*SRATE][i*STRIDE*SRATE:i*STRIDE*SRATE+LEN_INPUT*SRATE]) / np.nanmean(vals['nMBP'][-LEN_PER_POST*SRATE:][i*STRIDE*SRATE:i*STRIDE*SRATE+LEN_INPUT*SRATE]) - 1\n",
    "                if np.isnan(del_mbp):\n",
    "                    nan_mbp = nan_mbp + 1\n",
    "                y_mbp.append(del_mbp)\n",
    "                \n",
    "                age.append(int(row_demo['age'].values[0]))\n",
    "                if row_demo['sex'].values[0]=='F':\n",
    "                    gender.append(1)\n",
    "                else:\n",
    "                    gender.append(0)\n",
    "                x.append([ppg_input, ecg_input])\n",
    "                x_rftn.append(rftn)\n",
    "                y.append(del_hr)\n",
    "                c.append(caseid)\n",
    "\n",
    "    print(f'\\ndumping cache of df_preprocess {f_num}/{len(caseids)}', end='...')\n",
    "\n",
    "\n",
    "    df_preprocess.reset_index(drop=True, inplace=True)    \n",
    "    pickle.dump(df_preprocess, open(f'cache/preprocess/df_preprocess_{hyper_path}', 'wb'))\n",
    "    print('dumping success')\n",
    "    \n",
    "    # dataset\n",
    "    x = np.array(x, np.float32)\n",
    "    x_rftn = np.array(x_rftn, np.float32)\n",
    "    y = np.array(y, np.float32)\n",
    "    y_mbp = np.array(y_mbp)\n",
    "    c = np.array(c)\n",
    "    age = np.array(age, int)\n",
    "    gender = np.array(gender, int)\n",
    "\n",
    "    # 알맞게 input 변환\n",
    "    x = np.transpose(x, [0,2,1])\n",
    "\n",
    "    print('after concatenate + transpose')\n",
    "    print('x shape:', x.shape)\n",
    "    print('rftn shape:', x_rftn.shape)\n",
    "    print('y shape:', y.shape)\n",
    "    print('caseid num: ', len(c))\n",
    "    print(f'input path: {input_path}\\n')\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "\n",
    "    # train, test set\n",
    "    caseids = list(np.unique(c))\n",
    "    if len(caseids) != len(c):\n",
    "        print('overlapped caseid exists')\n",
    "    random.shuffle(caseids)\n",
    "\n",
    "    ntest = max(1, int(len(caseids) * 0.1))\n",
    "    ntrain = len(caseids) - ntest\n",
    "\n",
    "    caseid_train = caseids[:ntrain]\n",
    "    caseid_test = caseids[ntrain:]\n",
    "\n",
    "    print('전체 caseid 수: {}'.format(len(caseids)))\n",
    "    print('train caseid 수: {}, test caseid 수: {}'.format(len(caseid_train), len(caseid_test)))\n",
    "\n",
    "    pickle.dump(caseid_train, open(f'../DL_model/pd/caseid_train_{hyper_path}','wb'))\n",
    "    pickle.dump(caseid_test, open(f'../DL_model/pd/caseid_test_{hyper_path}','wb'))\n",
    "\n",
    "    train_mask = np.isin(c, caseid_train)\n",
    "    test_mask = np.isin(c, caseid_test)\n",
    "\n",
    "    x_train = x[train_mask]\n",
    "    x_test = x[test_mask]\n",
    "    y_train = y[train_mask]\n",
    "    y_test = y[test_mask]\n",
    "    mbp_train = y_mbp[train_mask]\n",
    "    mbp_test = y_mbp[test_mask]\n",
    "    rftn_train = x_rftn[train_mask]\n",
    "    rftn_test = x_rftn[test_mask]\n",
    "    c_train = c[train_mask]\n",
    "    c_test = c[test_mask]\n",
    "\n",
    "    age_train = age[train_mask]\n",
    "    age_test = age[test_mask]\n",
    "    gender_train = gender[train_mask]\n",
    "    gender_test = gender[test_mask]\n",
    "\n",
    "\n",
    "    # 저장하기\n",
    "    print('saving...', end='', flush=True)\n",
    "    np.savez_compressed(input_path+'x_train.npz', x_train)\n",
    "    np.savez_compressed(input_path+'x_test.npz', x_test)\n",
    "    np.savez_compressed(input_path+'rftn_train.npz', rftn_train)\n",
    "    np.savez_compressed(input_path+'rftn_test.npz', rftn_test)\n",
    "    np.savez_compressed(input_path+'y_train.npz', y_train)\n",
    "    np.savez_compressed(input_path+'y_test.npz', y_test)\n",
    "    np.savez_compressed(input_path+'mbp_train.npz', mbp_train)\n",
    "    np.savez_compressed(input_path+'mbp_test.npz', mbp_test)\n",
    "    np.savez_compressed(input_path+'c_train.npz', c_train)\n",
    "    np.savez_compressed(input_path+'c_test.npz', c_test)\n",
    "\n",
    "    np.savez_compressed(input_path+'age_train.npz', age_train)\n",
    "    np.savez_compressed(input_path+'age_test.npz', age_test)\n",
    "    np.savez_compressed(input_path+'gender_train.npz', gender_train)\n",
    "    np.savez_compressed(input_path+'gender_test.npz', gender_test)\n",
    "\n",
    "    print('done', flush=True)\n",
    "    print('\\nsize of training set:', len(x_train))\n",
    "    print('size of test set:', len(x_test)) \n",
    "    print(f'num of hr value nan : {nan_hr}')\n",
    "    print(f'num of mbp value nan : {nan_mbp}')\n",
    "    print(f'num of rftn value nan : {nan_rftn}')\n",
    "    print(f'num of low baseline bp : {len(low_mbp)}')\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "    print(f'total elapsed time for preprocess: {time.time() - start:.2f}s\\n')\n",
    "\n",
    "\n",
    "    # 전처리 통과 비율 출력\n",
    "    nl_pass, l_pass = [], []\n",
    "    for idx, row in df_preprocess.iterrows():\n",
    "        for i in range(n_aug):\n",
    "            nl_pass.append(row[f'{i+1}'][0])\n",
    "        #l_pass.append(row['2'][0])\n",
    "\n",
    "    print(f'전처리 성공 비율 : intubation 직전 {np.mean(nl_pass)*100:.2f}%') #, intubation 직후 {np.mean(l_pass)*100:.2f}%')\n",
    "    print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afffe4b3-0f5a-4b8b-b584-e54ce8584252",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRATE = 300\n",
    "ECG_FILT = '3rd-bandpass'\n",
    "PPG_FILT = 'lowess'\n",
    "CORR_THRES = 0.9 # cutoff\n",
    "LEN_INPUT = 30 # length of the input\n",
    "STRIDE = 1\n",
    "LEN_PER_PRE = 30 # before intubation\n",
    "LEN_PER_POST = 30\n",
    "\n",
    "file_path = f'vital_to_np_pd_{SRATE}Hz'\n",
    "\n",
    "preprocess(file_path, SRATE, ECG_FILT, PPG_FILT, CORR_THRES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd3220-8f39-4c5c-8cf0-dbfca4ceecbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
