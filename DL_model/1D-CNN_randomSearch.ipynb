{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66fd7e-43ab-4acc-a260-5e086232268c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import losses, metrics\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, BatchNormalization, Dropout, Activation\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "import tensorflow as tf\n",
    "import os, sys, pickle\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afdbb9-8128-4e8f-8dcc-55e5bd2184fe",
   "metadata": {},
   "source": [
    "# GPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f5990a-1951-44b6-ba7f-4cb6c9e1ff2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-17T02:50:52.519301Z",
     "iopub.status.busy": "2022-01-17T02:50:52.519138Z",
     "iopub.status.idle": "2022-01-17T02:50:52.522858Z",
     "shell.execute_reply": "2022-01-17T02:50:52.522183Z",
     "shell.execute_reply.started": "2022-01-17T02:50:52.519279Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70812fe3-9e51-451c-8075-9f0e8f7f0b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1741e5-8d15-439f-8c98-e5584d241cd3",
   "metadata": {},
   "source": [
    "# 1. Loading input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0b0c42-49c7-400b-a249-cd30cb4e90a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-17T02:50:53.839241Z",
     "iopub.status.busy": "2022-01-17T02:50:53.838764Z",
     "iopub.status.idle": "2022-01-17T02:50:55.853074Z",
     "shell.execute_reply": "2022-01-17T02:50:55.852588Z",
     "shell.execute_reply.started": "2022-01-17T02:50:53.839191Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train...done\n",
      "x_train shape: (14182, 2000, 2)\n",
      "x_test.shape: (1791, 2000, 2)\n",
      "x_val.shape: (1571, 2000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "MAX_CASES = 2000\n",
    "SEGLEN_IN_SEC = 20\n",
    "SRATE = 100\n",
    "LEN_INPUT = 20\n",
    "OVERLAP = 10\n",
    "LEN_PER_PRE = 60\n",
    "LEN_PER_POST = 120\n",
    "\n",
    "\n",
    "print('loading train...', flush=True, end='')\n",
    "\n",
    "# x를 loading해서 (batch_size, step, channel)\n",
    "input_path = f\"../DL_model/dataset/ne{LEN_PER_PRE}s-e{LEN_PER_POST}s-len{LEN_INPUT}-{OVERLAP}/\"\n",
    "x_train = np.load(input_path+'x_train.npz', allow_pickle=True)['arr_0']\n",
    "x_test = np.load(input_path+'x_test.npz', allow_pickle=True)['arr_0']\n",
    "x_val = np.load(input_path+'x_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "tss_train = np.load(input_path+'tss_train.npz')['arr_0']\n",
    "tss_test = np.load(input_path+'tss_test.npz')['arr_0']\n",
    "tss_val = np.load(input_path+'tss_val.npz')['arr_0']\n",
    "cisa_train = np.load(input_path+'cisa_train.npz')['arr_0']\n",
    "cisa_test = np.load(input_path+'cisa_test.npz')['arr_0']\n",
    "cisa_val = np.load(input_path+'cisa_val.npz')['arr_0']\n",
    "\n",
    "gender_train = np.load(input_path+'gender_train.npz', allow_pickle=True)['arr_0']\n",
    "gender_test = np.load(input_path+'gender_test.npz', allow_pickle=True)['arr_0']\n",
    "gender_val = np.load(input_path+'gender_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "age_train = np.load(input_path+'age_train.npz', allow_pickle=True)['arr_0']\n",
    "age_test = np.load(input_path+'age_test.npz', allow_pickle=True)['arr_0']\n",
    "age_val = np.load(input_path+'age_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "\n",
    "print('done', flush=True)\n",
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test.shape:', x_test.shape)\n",
    "print('x_val.shape:', x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfcdfab7-934b-4686-b59e-11ab0108a447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-17T02:50:58.204760Z",
     "iopub.status.busy": "2022-01-17T02:50:58.204265Z",
     "iopub.status.idle": "2022-01-17T02:50:58.210900Z",
     "shell.execute_reply": "2022-01-17T02:50:58.209874Z",
     "shell.execute_reply.started": "2022-01-17T02:50:58.204707Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = 'TSS'\n",
    "\n",
    "if mode == 'TSS':\n",
    "    # tss를 output으로 할 경우\n",
    "    y_train = tss_train\n",
    "    y_val = tss_val\n",
    "    y_test = tss_test\n",
    "\n",
    "elif mode == 'CISA':\n",
    "    # cisa를 output으로 할 경우\n",
    "    y_train = cisa_train\n",
    "    y_val = cisa_val\n",
    "    y_test = cisa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e8845-7d15-4713-8b3e-5a4dfb6ed4de",
   "metadata": {},
   "source": [
    "# 2. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5847ab-4bb4-47ea-a32a-1761437e121d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-17T02:51:49.332290Z",
     "iopub.status.busy": "2022-01-17T02:51:49.331782Z",
     "iopub.status.idle": "2022-01-17T02:51:50.418819Z",
     "shell.execute_reply": "2022-01-17T02:51:50.418165Z",
     "shell.execute_reply.started": "2022-01-17T02:51:49.332241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making test settings...done\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "nfold = 1  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 300\n",
    "rootdir = f\"randomSearch/TSS/CNN_4layers_Reg_{nfold}fold_test{ntest}\"\n",
    "\n",
    "predirs = []\n",
    "for root, dirs, files in os.walk(rootdir):  # 하위 대상들을 recursive 하게 긁어옴\n",
    "    for filename in dirs:\n",
    "        predirs.append(filename)\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "\n",
    "# test_settings\n",
    "test_settings_1, test_settings_2, test_settings_3 = [], [], []\n",
    "\n",
    "\n",
    "# hyperparamters\n",
    "#num_nodes = [64, 64, 64] #, 64, 64, 64]\n",
    "#kernel_size = 10\n",
    "pool_size = 2\n",
    "\n",
    "#dense_node = 32\n",
    "#dropout_rate = 0.2\n",
    "learning_rate = 0.002\n",
    "\n",
    "# hyperparamters pool\n",
    "num_opts = [32, 64, 128, 256] # num of filters(kernel)\n",
    "stride_opts = [1,1,1,1,1,2,2,2,2]\n",
    "kernel_opts = range(3,9,2) # kernel size\n",
    "dropout_opts  = [0, 0.1, 0.2, 0.3, 0.4, 0.5] # dropout rate\n",
    "dense_opts = [0, 8, 16, 32, 64]\n",
    "globalpool_opts = ['max','ave']\n",
    "BATCH_SIZE = [512, 1024]\n",
    "\n",
    "\n",
    "print('start making test settings...', end='', flush=True)\n",
    "# test settings\n",
    "for num_l1 in num_opts:\n",
    "    for num_l2 in num_opts:\n",
    "        for num_l3 in num_opts:\n",
    "            for num_l4 in num_opts:\n",
    "                for kernel_l1 in kernel_opts:\n",
    "                    for kernel_l2 in kernel_opts:\n",
    "                        for kernel_l3 in kernel_opts:\n",
    "                            for kernel_l4 in kernel_opts:\n",
    "                                test_settings_1.append([num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4])\n",
    "\n",
    "for dense_node in dense_opts:\n",
    "    for dropout_cnn in dropout_opts:\n",
    "        for dropout_fc in dropout_opts:\n",
    "            for globalpool_opt in globalpool_opts:\n",
    "                for batch_size in BATCH_SIZE:\n",
    "                    for conv_double in [True, False]:\n",
    "                        test_settings_2.append([dense_node, dropout_cnn, dropout_fc, globalpool_opt, batch_size, conv_double])                                   \n",
    "\n",
    "for stride_l1 in stride_opts:\n",
    "    for stride_l2 in stride_opts:\n",
    "        for stride_l3 in stride_opts:\n",
    "            for stride_l4 in stride_opts:\n",
    "                for stride_l5 in stride_opts:\n",
    "                    for num_l5 in num_opts:\n",
    "                        for kernel_l5 in kernel_opts:\n",
    "                            test_settings_3.append([stride_l1, stride_l2, stride_l3, stride_l4, stride_l5, num_l5, kernel_l5])\n",
    "                        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f7135-75f1-4a1b-9860-18fb0a9333f7",
   "metadata": {},
   "source": [
    "# 3. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0de30-e0ce-49c8-83a9-3d512d5e438a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058a1aa-0441-41b0-82cf-f4198f0e3c38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "\n",
    "\n",
    "# random search for hyperparameter\n",
    "ntrial = 200\n",
    "train_errs, val_errs = [] ,[]\n",
    "#test_roc, test_prc = [], []\n",
    "test_rmse, test_mae = [], []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    test_setting_1 = random.choice(test_settings_1)\n",
    "    test_setting_2 = random.choice(test_settings_2)\n",
    "    test_setting_3 = random.choice(test_settings_3)\n",
    "        \n",
    "        \n",
    "    # test_setting\n",
    "    num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4 = test_setting_1\n",
    "    dense_node, dropout_cnn, dropout_fc, globalpool_opt, batch_size, conv_double = test_setting_2\n",
    "    stride_l1, stride_l2, stride_l3, stride_l4, stride_l5, num_l5, kernel_l5 = test_setting_3\n",
    "    \n",
    "    \n",
    "    # total conv layers of the model\n",
    "    n_conv = random.choice([2,3,4])\n",
    "    \n",
    "    if n_conv==2:\n",
    "        num_l3,kernel_l3,stride_l3 = 0,0,0\n",
    "        num_l4,kernel_l4,stride_l4 = 0,0,0\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0\n",
    "    \n",
    "    if n_conv==3:\n",
    "        num_l4,kernel_l4,stride_l4 = 0,0,0\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0\n",
    "        \n",
    "    if n_conv==4:\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0    \n",
    "    \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = 'batch={},c1={},c2={},c3={},c4={},filt1={},filt2={},filt3={},filt4={},str1={},str2={},str3={},str4={}, conv_double={},globalpool={},dropout={},dnodes={},dropout={}'.format(batch_size, num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4,stride_l1,stride_l2,stride_l3,stride_l4,conv_double, globalpool_opt, dropout_cnn, dense_node, dropout_fc)\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "    weightcache = \"{}/weights.hdf5\".format(odir)        \n",
    "\n",
    "    \n",
    "    with tf.device('/gpu:1'):\n",
    "        # build a model\n",
    "        model = Sequential()\n",
    "\n",
    "        act='relu'\n",
    "\n",
    "        # c1 layer\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l1, kernel_size=kernel_l1, strides=stride_l1, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l1, kernel_size=kernel_l1, strides=stride_l1, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "\n",
    "        # c2 layer\n",
    "        if num_l1 == 512:\n",
    "            model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l2, kernel_size=kernel_l2, strides=stride_l2, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l2, kernel_size=kernel_l2, strides=stride_l2, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        \n",
    "        # c3 layer\n",
    "        if n_conv>2:\n",
    "            if num_l2 == 512:\n",
    "                model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "            if conv_double:\n",
    "                model.add(Conv1D(filters=num_l3, kernel_size=kernel_l3, strides=stride_l3, padding='same'))\n",
    "            model.add(Conv1D(filters=num_l3, kernel_size=kernel_l3, strides=stride_l3, padding='same', activation=act))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        \n",
    "        # c4 layer\n",
    "        if n_conv>3:\n",
    "            if num_l3 == 512:\n",
    "                model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "            if conv_double:\n",
    "                model.add(Conv1D(filters=num_l4, kernel_size=kernel_l4, strides=stride_l4, padding='same'))\n",
    "            model.add(Conv1D(filters=num_l4, kernel_size=kernel_l4, strides=stride_l4, padding='same', activation=act))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "            \n",
    "        # c5 layer\n",
    "        if n_conv>4:\n",
    "            if num_l4 == 512:\n",
    "                model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "            if conv_double:\n",
    "                model.add(Conv1D(filters=num_l5, kernel_size=kernel_l5, strides=stride_l5, padding='same'))\n",
    "            model.add(Conv1D(filters=num_l5, kernel_size=kernel_l5, strides=stride_l5,padding='same', activation=act))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))            \n",
    "            \n",
    "\n",
    "\n",
    "        # global이냐 flatten이냐는 따로 모델 나눠야 할듯\n",
    "        if globalpool_opt == 'max':\n",
    "            model.add(GlobalMaxPool1D())\n",
    "        elif globalpool_opt == 'ave':\n",
    "            model.add(GlobalAveragePooling1D())\n",
    "            \n",
    "            \n",
    "        if dense_node != 0:\n",
    "            model.add(Dropout(dropout_cnn))\n",
    "            model.add(Dense(dense_node, activation='tanh'))\n",
    "        model.add(Dropout(dropout_fc))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "\n",
    "        # model 학습 설정\n",
    "        try:\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=[\"mean_absolute_error\"])\n",
    "            hist = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')])\n",
    "        except Exception as e:\n",
    "            print(f'error: {e}')\n",
    "            os.rmdir(odir)\n",
    "            #os.rename(odir,rootdir+'/error_{}'.format(odir_f))\n",
    "            itrial -= 1\n",
    "            test_mae.append(0)\n",
    "            test_rmse.append(0)\n",
    "            train_errs.append(-1)\n",
    "            val_errs.append(-1)\n",
    "            continue\n",
    "            \n",
    "\n",
    "    # 모델의 아키텍처 및 구조 저장\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    # test set에 대한 y_pred 계산\n",
    "    model.load_weights(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "    y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "    # auroc 계산\n",
    "    #false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred)\n",
    "    #roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    #test_auc.append(roc_auc)\n",
    "    \n",
    "    # RMSE 계산\n",
    "    model_err = metrics.RootMeanSquaredError()\n",
    "    model_err.update_state(y_test, y_pred)\n",
    "    rmse_val = model_err.result().numpy()\n",
    "    test_rmse.append(rmse_val)\n",
    "    \n",
    "    # MAE 계산\n",
    "    model_err = metrics.MeanAbsoluteError()\n",
    "    model_err.update_state(y_test, y_pred)\n",
    "    mae_val = model_err.result().numpy()\n",
    "    test_mae.append(mae_val)\n",
    "    \n",
    "    \n",
    "    # acc 계산\n",
    "    #acc_val = np.mean((y_pred*10>=5)==y_test_bin)\n",
    "    #test_acc.append(acc_val)\n",
    "    \n",
    "    # rename\n",
    "    os.rename(odir, f'{rootdir}/mae{mae_val:.3f}_rmse{rmse_val:.3f}_{odir_f}')\n",
    "\n",
    "    # train 과정에서의 err\n",
    "    train_err = min(hist.history['loss'])\n",
    "    val_err = min(hist.history['val_loss'])\n",
    "\n",
    "    val_errs.append(val_err)\n",
    "    train_errs.append(train_err)\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "max_idx = test_roc.index(max(test_auc))\n",
    "print('\\nBest Model roc:{:.4f}, info: {}'.format(test_roc(max_idx), random_settings(max_idx)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c3d46-d870-4d7a-b41e-261f01d12e42",
   "metadata": {},
   "source": [
    "## Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966cf68f-6c01-4cd5-a424-3e8564c04378",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # model 학습 설정\n",
    "        try:\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=[\"acc\", tf.keras.metrics.AUC()])\n",
    "            hist = model.fit(x_train_ecg, y_train_bin, sample_weight=train_w_samp3, validation_data=(x_val_ecg, y_val_bin, val_w_samp3), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')])\n",
    "        except:\n",
    "            os.rmdir(odir)\n",
    "            #os.rename(odir,rootdir+'/error_{}'.format(odir_f))\n",
    "            itrial -= 1\n",
    "            test_roc.append(0)\n",
    "            test_acc.append(0)\n",
    "            test_prc.append(0)\n",
    "            train_errs.append(-1)\n",
    "            val_errs.append(-1)\n",
    "            continue\n",
    "            \n",
    "            \n",
    "    # 모델의 아키텍처 및 구조 저장\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    # test set에 대한 y_pred 계산\n",
    "    model.load_weights(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "    y_pred = model.predict(x_test_ecg).flatten()\n",
    "\n",
    "    \n",
    "    # acc 계산\n",
    "    acc = metrics.Accuracy()\n",
    "    acc.update_state(y_pred>=0.5, y_test_bin, sample_weight=test_w_samp3)\n",
    "    acc_val = acc.result().numpy()\n",
    "    test_acc.append(acc_val)\n",
    "    \n",
    "    # auroc 계산\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred, sample_weight=test_w_samp3)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_roc.append(roc_auc)\n",
    "\n",
    "    # auprc \n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin, y_pred, sample_weight=test_w_samp3)\n",
    "    prc_auc = auc(recall, precision)\n",
    "    test_prc.append(prc_auc)\n",
    "\n",
    "    \n",
    "    # rename\n",
    "    os.rename(odir, rootdir+'/roc{:.4f}_prc{:.4f}_{}_acc{:.2f}'.format(roc_auc, prc_auc, odir_f, acc_val))\n",
    "\n",
    "    # train 과정에서의 err\n",
    "    train_err = min(hist.history['loss'])\n",
    "    val_err = min(hist.history['val_loss'])\n",
    "\n",
    "    val_errs.append(val_err)\n",
    "    train_errs.append(train_err)\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "max_idx = test_roc.index(max(test_auc))\n",
    "print('\\nBest Model roc:{:.4f}, info: {}'.format(test_roc(max_idx), random_settings(max_idx)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "painstudy_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
