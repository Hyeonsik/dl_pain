{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c66fd7e-43ab-4acc-a260-5e086232268c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T17:21:00.220898Z",
     "iopub.status.busy": "2022-02-12T17:21:00.220263Z",
     "iopub.status.idle": "2022-02-12T17:21:02.400031Z",
     "shell.execute_reply": "2022-02-12T17:21:02.399405Z",
     "shell.execute_reply.started": "2022-02-12T17:21:00.220757Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import losses, metrics\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, BatchNormalization, Dropout, Activation\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "import tensorflow as tf\n",
    "import os, sys, pickle\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afdbb9-8128-4e8f-8dcc-55e5bd2184fe",
   "metadata": {},
   "source": [
    "# GPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f5990a-1951-44b6-ba7f-4cb6c9e1ff2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T17:21:02.401293Z",
     "iopub.status.busy": "2022-02-12T17:21:02.401037Z",
     "iopub.status.idle": "2022-02-12T17:21:02.404300Z",
     "shell.execute_reply": "2022-02-12T17:21:02.403786Z",
     "shell.execute_reply.started": "2022-02-12T17:21:02.401269Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1741e5-8d15-439f-8c98-e5584d241cd3",
   "metadata": {},
   "source": [
    "# 1. Loading input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0b0c42-49c7-400b-a249-cd30cb4e90a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T17:21:04.082933Z",
     "iopub.status.busy": "2022-02-12T17:21:04.082493Z",
     "iopub.status.idle": "2022-02-12T17:21:05.376065Z",
     "shell.execute_reply": "2022-02-12T17:21:05.375480Z",
     "shell.execute_reply.started": "2022-02-12T17:21:04.082890Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train...done\n",
      "x_train shape: (9135, 2000, 2)\n",
      "x_test.shape: (1159, 2000, 2)\n",
      "x_val.shape: (1023, 2000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "MAX_CASES = 2000\n",
    "SEGLEN_IN_SEC = 20\n",
    "SRATE = 100\n",
    "LEN_INPUT = 20\n",
    "OVERLAP = 10\n",
    "LEN_PER_PRE = 60\n",
    "LEN_PER_POST = 60\n",
    "\n",
    "\n",
    "print('loading train...', flush=True, end='')\n",
    "\n",
    "# x를 loading해서 (batch_size, step, channel)\n",
    "input_path = f\"../DL_model/dataset/ne{LEN_PER_PRE}s-e{LEN_PER_POST}s-len{LEN_INPUT}-{OVERLAP}/\"\n",
    "x_train = np.load(input_path+'x_train.npz', allow_pickle=True)['arr_0']\n",
    "x_test = np.load(input_path+'x_test.npz', allow_pickle=True)['arr_0']\n",
    "x_val = np.load(input_path+'x_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "tss_train = np.load(input_path+'tss_train.npz')['arr_0']\n",
    "tss_test = np.load(input_path+'tss_test.npz')['arr_0']\n",
    "tss_val = np.load(input_path+'tss_val.npz')['arr_0']\n",
    "cisa_train = np.load(input_path+'cisa_train.npz')['arr_0']\n",
    "cisa_test = np.load(input_path+'cisa_test.npz')['arr_0']\n",
    "cisa_val = np.load(input_path+'cisa_val.npz')['arr_0']\n",
    "\n",
    "gender_train = np.load(input_path+'gender_train.npz', allow_pickle=True)['arr_0']\n",
    "gender_test = np.load(input_path+'gender_test.npz', allow_pickle=True)['arr_0']\n",
    "gender_val = np.load(input_path+'gender_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "age_train = np.load(input_path+'age_train.npz', allow_pickle=True)['arr_0']\n",
    "age_test = np.load(input_path+'age_test.npz', allow_pickle=True)['arr_0']\n",
    "age_val = np.load(input_path+'age_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "\n",
    "print('done', flush=True)\n",
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test.shape:', x_test.shape)\n",
    "print('x_val.shape:', x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98bc2f-bdf8-4400-9f0d-4612c232c2d1",
   "metadata": {},
   "source": [
    "## Input, output settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b8f66d5-d6dd-4ec1-922b-1dc634dc3be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T17:21:05.668746Z",
     "iopub.status.busy": "2022-02-12T17:21:05.668405Z",
     "iopub.status.idle": "2022-02-12T17:21:05.673685Z",
     "shell.execute_reply": "2022-02-12T17:21:05.672969Z",
     "shell.execute_reply.started": "2022-02-12T17:21:05.668713Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input mode\n",
    "mode_in = 'PPG+ECG'\n",
    "\n",
    "if mode_in == 'PPG':\n",
    "    x_train = x_train[:,:,0:1]\n",
    "    x_test = x_test[:,:,0:1]\n",
    "    x_val = x_val[:,:,0:1]\n",
    "elif mode_in == 'ECG':\n",
    "    x_train = x_train[:,:,1:2]\n",
    "    x_test = x_test[:,:,1:2]\n",
    "    x_val = x_val[:,:,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfcdfab7-934b-4686-b59e-11ab0108a447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T17:21:07.011863Z",
     "iopub.status.busy": "2022-02-12T17:21:07.011447Z",
     "iopub.status.idle": "2022-02-12T17:21:07.016698Z",
     "shell.execute_reply": "2022-02-12T17:21:07.015828Z",
     "shell.execute_reply.started": "2022-02-12T17:21:07.011823Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = 'TSS'\n",
    "\n",
    "if mode == 'TSS':\n",
    "    # tss를 output으로 할 경우\n",
    "    y_train = tss_train\n",
    "    y_val = tss_val\n",
    "    y_test = tss_test\n",
    "\n",
    "elif mode == 'CISA':\n",
    "    # cisa를 output으로 할 경우\n",
    "    y_train = cisa_train\n",
    "    y_val = cisa_val\n",
    "    y_test = cisa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da180113-aec2-4894-af6d-115036200d93",
   "metadata": {},
   "source": [
    "## Sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4838805-26d2-4ef8-94f3-c38da8a24148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T17:21:09.893822Z",
     "iopub.status.busy": "2022-02-12T17:21:09.893375Z",
     "iopub.status.idle": "2022-02-12T17:21:09.913998Z",
     "shell.execute_reply": "2022-02-12T17:21:09.913258Z",
     "shell.execute_reply.started": "2022-02-12T17:21:09.893782Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<train set>\n",
      "sample weight for no pain: 1.42, moderate pain: 3.36\n",
      "sample weight for no pain: 1.29, severe pain: 4.44\n",
      "<val set>\n",
      "sample weight for no pain: 1.52, moderate pain: 2.94\n",
      "sample weight for no pain: 1.37, severe pain: 3.69\n",
      "<test set>\n",
      "sample weight for no pain: 1.43, moderate pain: 3.34\n",
      "sample weight for no pain: 1.28, severe pain: 4.60\n"
     ]
    }
   ],
   "source": [
    "# 2 class에 대한 sample weight\n",
    "y_train_bin = y_train > 0\n",
    "y_val_bin = y_val > 0\n",
    "y_test_bin = y_test > 0\n",
    "\n",
    "y_train_bin2 = y_train > 0.2\n",
    "y_val_bin2 = y_val > 0.2\n",
    "y_test_bin2 = y_test > 0.2\n",
    "\n",
    "\n",
    "train_w_samp2 = np.ones(shape=(len(y_train),))\n",
    "train_w_samp2[y_train_bin>0]= len(y_train) / np.sum(y_train_bin)\n",
    "train_w_samp2[y_train_bin==0]= len(y_train) / np.sum(~y_train_bin)\n",
    "\n",
    "train_w_samp2_2 = np.ones(shape=(len(y_train),))\n",
    "train_w_samp2_2[y_train_bin2>0]= len(y_train) / np.sum(y_train_bin2)\n",
    "train_w_samp2_2[y_train_bin2==0]= len(y_train) / np.sum(~y_train_bin2)\n",
    "\n",
    "print('<train set>')\n",
    "print('sample weight for no pain: {:.2f}, moderate pain: {:.2f}'\n",
    "      .format(len(y_train) / np.sum(~y_train_bin), len(y_train) / np.sum(y_train_bin)))\n",
    "print('sample weight for no pain: {:.2f}, severe pain: {:.2f}'\n",
    "      .format(len(y_train) / np.sum(~y_train_bin2), len(y_train) / np.sum(y_train_bin2)))\n",
    "\n",
    "# 2 class에 대한 sample weight\n",
    "val_w_samp2 = np.ones(shape=(len(y_val),))\n",
    "val_w_samp2[y_val_bin>0]= len(y_val) / np.sum(y_val_bin)\n",
    "val_w_samp2[y_val_bin==0]= len(y_val) / np.sum(~y_val_bin)\n",
    "\n",
    "val_w_samp2_2 = np.ones(shape=(len(y_val),))\n",
    "val_w_samp2_2[y_val_bin2>0]= len(y_val) / np.sum(y_val_bin2)\n",
    "val_w_samp2_2[y_val_bin2==0]= len(y_val) / np.sum(~y_val_bin2)\n",
    "\n",
    "print('<val set>')\n",
    "print('sample weight for no pain: {:.2f}, moderate pain: {:.2f}'\n",
    "      .format(len(y_val) / np.sum(~y_val_bin), len(y_val) / np.sum(y_val_bin)))\n",
    "print('sample weight for no pain: {:.2f}, severe pain: {:.2f}'\n",
    "      .format(len(y_val) / np.sum(~y_val_bin2), len(y_val) / np.sum(y_val_bin2)))\n",
    "\n",
    "# 2 class에 대한 sample weight\n",
    "test_w_samp2 = np.ones(shape=(len(y_test),))\n",
    "test_w_samp2[y_test_bin>0]= len(y_test) / np.sum(y_test_bin)\n",
    "test_w_samp2[y_test_bin==0]= len(y_test) / np.sum(~y_test_bin)\n",
    "\n",
    "test_w_samp2_2 = np.ones(shape=(len(y_test),))\n",
    "test_w_samp2_2[y_test_bin2>0]= len(y_test) / np.sum(y_test_bin2)\n",
    "test_w_samp2_2[y_test_bin2==0]= len(y_test) / np.sum(~y_test_bin2)\n",
    "\n",
    "print('<test set>')\n",
    "print('sample weight for no pain: {:.2f}, moderate pain: {:.2f}'\n",
    "      .format(len(y_test) / np.sum(~y_test_bin), len(y_test) / np.sum(y_test_bin)))\n",
    "print('sample weight for no pain: {:.2f}, severe pain: {:.2f}'\n",
    "      .format(len(y_test) / np.sum(~y_test_bin2), len(y_test) / np.sum(y_test_bin2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49391ffd-97e8-4f8f-a5a6-dcf8bcb6347e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T17:21:15.139258Z",
     "iopub.status.busy": "2022-02-12T17:21:15.138791Z",
     "iopub.status.idle": "2022-02-12T17:21:15.143852Z",
     "shell.execute_reply": "2022-02-12T17:21:15.142847Z",
     "shell.execute_reply.started": "2022-02-12T17:21:15.139213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_w_samp = train_w_samp2\n",
    "val_w_samp = val_w_samp2\n",
    "test_w_samp = test_w_samp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e8845-7d15-4713-8b3e-5a4dfb6ed4de",
   "metadata": {},
   "source": [
    "# 2. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a5847ab-4bb4-47ea-a32a-1761437e121d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T17:21:20.046181Z",
     "iopub.status.busy": "2022-02-12T17:21:20.045691Z",
     "iopub.status.idle": "2022-02-12T17:21:21.123034Z",
     "shell.execute_reply": "2022-02-12T17:21:21.122411Z",
     "shell.execute_reply.started": "2022-02-12T17:21:20.046137Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making test settings...done\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "nfold = 1  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 500\n",
    "rootdir = f\"randomSearch/TSS/CNN_{mode_in}_4layers_Reg_{nfold}fold_test{ntest}_w_samp0_lr\"\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "f = open(f'{rootdir}/README.txt', 'w')\n",
    "f.write(f'model: 1D CNN 4 layers, regression')\n",
    "f.write(f'input: {mode_in} (pre-intubation 120~60s, post-intubation 60~120s), output: {mode}')\n",
    "f.close()\n",
    "    \n",
    "\n",
    "# test_settings\n",
    "test_settings_1, test_settings_2, test_settings_3 = [], [], []\n",
    "\n",
    "\n",
    "# hyperparamters\n",
    "#num_nodes = [64, 64, 64] #, 64, 64, 64]\n",
    "#kernel_size = 10\n",
    "pool_size = 2\n",
    "\n",
    "#dense_node = 32\n",
    "#dropout_rate = 0.2\n",
    "learning_rate = 0.001\n",
    "\n",
    "# hyperparamters pool\n",
    "num_opts = [32, 64, 128, 256] # num of filters(kernel)\n",
    "stride_opts = [1,1,1,1,1,2,2,2,2]\n",
    "kernel_opts = range(3,9,2) # kernel size\n",
    "dropout_opts  = [0, 0.1, 0.2, 0.3, 0.4, 0.5] # dropout rate\n",
    "dense_opts = [0, 8, 16, 32, 64, 128]\n",
    "globalpool_opts = ['max','ave']\n",
    "BATCH_SIZE = [512, 1024]\n",
    "\n",
    "\n",
    "print('start making test settings...', end='', flush=True)\n",
    "# test settings\n",
    "for num_l1 in num_opts:\n",
    "    for num_l2 in num_opts:\n",
    "        for num_l3 in num_opts:\n",
    "            for num_l4 in num_opts:\n",
    "                for kernel_l1 in kernel_opts:\n",
    "                    for kernel_l2 in kernel_opts:\n",
    "                        for kernel_l3 in kernel_opts:\n",
    "                            for kernel_l4 in kernel_opts:\n",
    "                                test_settings_1.append([num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4])\n",
    "\n",
    "for dense_node in dense_opts:\n",
    "    for dropout_cnn in dropout_opts:\n",
    "        for dropout_fc in dropout_opts:\n",
    "            for globalpool_opt in globalpool_opts:\n",
    "                for batch_size in BATCH_SIZE:\n",
    "                    for conv_double in [True, False]:\n",
    "                        for learning_rate in [0.001, 0.002, 0.0005]:\n",
    "                            test_settings_2.append([dense_node, dropout_cnn, dropout_fc, globalpool_opt, batch_size, conv_double, learning_rate])                                   \n",
    "\n",
    "for stride_l1 in stride_opts:\n",
    "    for stride_l2 in stride_opts:\n",
    "        for stride_l3 in stride_opts:\n",
    "            for stride_l4 in stride_opts:\n",
    "                for stride_l5 in stride_opts:\n",
    "                    for num_l5 in num_opts:\n",
    "                        for kernel_l5 in kernel_opts:\n",
    "                            test_settings_3.append([stride_l1, stride_l2, stride_l3, stride_l4, stride_l5, num_l5, kernel_l5])\n",
    "                        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f7135-75f1-4a1b-9860-18fb0a9333f7",
   "metadata": {},
   "source": [
    "# 3. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0de30-e0ce-49c8-83a9-3d512d5e438a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058a1aa-0441-41b0-82cf-f4198f0e3c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T17:21:24.878699Z",
     "iopub.status.busy": "2022-02-12T17:21:24.878197Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random search 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 02:21:25.083577: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-13 02:21:25.889977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30267 MB memory:  -> device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 02:21:31.004972: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101\n",
      "2022-02-13 02:21:31.450540: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 1.3200 - mean_absolute_error: 0.5619\n",
      "Epoch 00001: val_loss improved from inf to 0.68348, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=256,c3=64,c4=0,filt1=3,filt2=5,filt3=5,filt4=0,str1=1,         str2=1,str3=2,str4=0, conv_double=True,globalpool=ave,dropout=0,dnodes=32,dropout=0.5,lr=0.001/weights.hdf5\n",
      "18/18 [==============================] - 9s 260ms/step - loss: 1.3200 - mean_absolute_error: 0.5619 - val_loss: 0.6835 - val_mean_absolute_error: 0.5062\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.3123 - mean_absolute_error: 0.2998\n",
      "Epoch 00002: val_loss improved from 0.68348 to 0.17565, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=256,c3=64,c4=0,filt1=3,filt2=5,filt3=5,filt4=0,str1=1,         str2=1,str3=2,str4=0, conv_double=True,globalpool=ave,dropout=0,dnodes=32,dropout=0.5,lr=0.001/weights.hdf5\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 0.3123 - mean_absolute_error: 0.2998 - val_loss: 0.1756 - val_mean_absolute_error: 0.1775\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1370 - mean_absolute_error: 0.1940\n",
      "Epoch 00003: val_loss did not improve from 0.17565\n",
      "18/18 [==============================] - 3s 162ms/step - loss: 0.1370 - mean_absolute_error: 0.1940 - val_loss: 0.6697 - val_mean_absolute_error: 0.4960\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0929 - mean_absolute_error: 0.1569\n",
      "Epoch 00004: val_loss improved from 0.17565 to 0.07250, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=256,c3=64,c4=0,filt1=3,filt2=5,filt3=5,filt4=0,str1=1,         str2=1,str3=2,str4=0, conv_double=True,globalpool=ave,dropout=0,dnodes=32,dropout=0.5,lr=0.001/weights.hdf5\n",
      "18/18 [==============================] - 3s 174ms/step - loss: 0.0929 - mean_absolute_error: 0.1569 - val_loss: 0.0725 - val_mean_absolute_error: 0.1359\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0808 - mean_absolute_error: 0.1445\n",
      "Epoch 00005: val_loss did not improve from 0.07250\n",
      "18/18 [==============================] - 3s 163ms/step - loss: 0.0808 - mean_absolute_error: 0.1445 - val_loss: 0.0845 - val_mean_absolute_error: 0.1951\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0763 - mean_absolute_error: 0.1411\n",
      "Epoch 00006: val_loss did not improve from 0.07250\n",
      "18/18 [==============================] - 3s 163ms/step - loss: 0.0763 - mean_absolute_error: 0.1411 - val_loss: 0.0889 - val_mean_absolute_error: 0.2057\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0720 - mean_absolute_error: 0.1389\n",
      "Epoch 00007: val_loss improved from 0.07250 to 0.05582, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=256,c3=64,c4=0,filt1=3,filt2=5,filt3=5,filt4=0,str1=1,         str2=1,str3=2,str4=0, conv_double=True,globalpool=ave,dropout=0,dnodes=32,dropout=0.5,lr=0.001/weights.hdf5\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 0.0720 - mean_absolute_error: 0.1389 - val_loss: 0.0558 - val_mean_absolute_error: 0.1513\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0685 - mean_absolute_error: 0.1364\n",
      "Epoch 00008: val_loss did not improve from 0.05582\n",
      "18/18 [==============================] - 3s 163ms/step - loss: 0.0685 - mean_absolute_error: 0.1364 - val_loss: 0.0682 - val_mean_absolute_error: 0.1768\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0687 - mean_absolute_error: 0.1354\n",
      "Epoch 00009: val_loss did not improve from 0.05582\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 0.0687 - mean_absolute_error: 0.1354 - val_loss: 0.0889 - val_mean_absolute_error: 0.2038\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0661 - mean_absolute_error: 0.1329\n",
      "Epoch 00010: val_loss improved from 0.05582 to 0.05124, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=256,c3=64,c4=0,filt1=3,filt2=5,filt3=5,filt4=0,str1=1,         str2=1,str3=2,str4=0, conv_double=True,globalpool=ave,dropout=0,dnodes=32,dropout=0.5,lr=0.001/weights.hdf5\n",
      "18/18 [==============================] - 3s 168ms/step - loss: 0.0661 - mean_absolute_error: 0.1329 - val_loss: 0.0512 - val_mean_absolute_error: 0.1461\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0654 - mean_absolute_error: 0.1331\n",
      "Epoch 00011: val_loss improved from 0.05124 to 0.05007, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=256,c3=64,c4=0,filt1=3,filt2=5,filt3=5,filt4=0,str1=1,         str2=1,str3=2,str4=0, conv_double=True,globalpool=ave,dropout=0,dnodes=32,dropout=0.5,lr=0.001/weights.hdf5\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 0.0654 - mean_absolute_error: 0.1331 - val_loss: 0.0501 - val_mean_absolute_error: 0.1430\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0668 - mean_absolute_error: 0.1357\n",
      "Epoch 00012: val_loss did not improve from 0.05007\n",
      "18/18 [==============================] - 3s 162ms/step - loss: 0.0668 - mean_absolute_error: 0.1357 - val_loss: 0.0574 - val_mean_absolute_error: 0.1134\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0653 - mean_absolute_error: 0.1304\n",
      "Epoch 00013: val_loss improved from 0.05007 to 0.04771, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=256,c3=64,c4=0,filt1=3,filt2=5,filt3=5,filt4=0,str1=1,         str2=1,str3=2,str4=0, conv_double=True,globalpool=ave,dropout=0,dnodes=32,dropout=0.5,lr=0.001/weights.hdf5\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 0.0653 - mean_absolute_error: 0.1304 - val_loss: 0.0477 - val_mean_absolute_error: 0.1189\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0660 - mean_absolute_error: 0.1342\n",
      "Epoch 00014: val_loss did not improve from 0.04771\n",
      "18/18 [==============================] - 3s 166ms/step - loss: 0.0660 - mean_absolute_error: 0.1342 - val_loss: 0.0485 - val_mean_absolute_error: 0.1145\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0643 - mean_absolute_error: 0.1330\n",
      "Epoch 00015: val_loss did not improve from 0.04771\n",
      "18/18 [==============================] - 3s 162ms/step - loss: 0.0643 - mean_absolute_error: 0.1330 - val_loss: 0.0493 - val_mean_absolute_error: 0.1059\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0647 - mean_absolute_error: 0.1317\n",
      "Epoch 00016: val_loss improved from 0.04771 to 0.04677, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=256,c3=64,c4=0,filt1=3,filt2=5,filt3=5,filt4=0,str1=1,         str2=1,str3=2,str4=0, conv_double=True,globalpool=ave,dropout=0,dnodes=32,dropout=0.5,lr=0.001/weights.hdf5\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 0.0647 - mean_absolute_error: 0.1317 - val_loss: 0.0468 - val_mean_absolute_error: 0.1315\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0640 - mean_absolute_error: 0.1317\n",
      "Epoch 00017: val_loss improved from 0.04677 to 0.04527, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=256,c3=64,c4=0,filt1=3,filt2=5,filt3=5,filt4=0,str1=1,         str2=1,str3=2,str4=0, conv_double=True,globalpool=ave,dropout=0,dnodes=32,dropout=0.5,lr=0.001/weights.hdf5\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 0.0640 - mean_absolute_error: 0.1317 - val_loss: 0.0453 - val_mean_absolute_error: 0.1231\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0637 - mean_absolute_error: 0.1327\n",
      "Epoch 00018: val_loss did not improve from 0.04527\n",
      "18/18 [==============================] - 3s 163ms/step - loss: 0.0637 - mean_absolute_error: 0.1327 - val_loss: 0.0462 - val_mean_absolute_error: 0.1257\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0639 - mean_absolute_error: 0.1330\n",
      "Epoch 00019: val_loss did not improve from 0.04527\n",
      "18/18 [==============================] - 3s 169ms/step - loss: 0.0639 - mean_absolute_error: 0.1330 - val_loss: 0.0546 - val_mean_absolute_error: 0.1053\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0634 - mean_absolute_error: 0.1291\n",
      "Epoch 00020: val_loss did not improve from 0.04527\n",
      "18/18 [==============================] - 3s 162ms/step - loss: 0.0634 - mean_absolute_error: 0.1291 - val_loss: 0.0457 - val_mean_absolute_error: 0.1178\n",
      "random search 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.0505 - mean_absolute_error: 0.5113\n",
      "Epoch 00001: val_loss improved from inf to 0.05730, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=32,c3=128,c4=0,filt1=7,filt2=5,filt3=7,filt4=0,str1=2,         str2=1,str3=1,str4=0, conv_double=False,globalpool=ave,dropout=0,dnodes=128,dropout=0.4,lr=0.002/weights.hdf5\n",
      "18/18 [==============================] - 2s 67ms/step - loss: 1.0505 - mean_absolute_error: 0.5113 - val_loss: 0.0573 - val_mean_absolute_error: 0.1297\n",
      "Epoch 2/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2403 - mean_absolute_error: 0.2544\n",
      "Epoch 00002: val_loss did not improve from 0.05730\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.2313 - mean_absolute_error: 0.2484 - val_loss: 0.0699 - val_mean_absolute_error: 0.1100\n",
      "Epoch 3/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.1192 - mean_absolute_error: 0.1843\n",
      "Epoch 00003: val_loss did not improve from 0.05730\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.1174 - mean_absolute_error: 0.1813 - val_loss: 0.0949 - val_mean_absolute_error: 0.1134\n",
      "Epoch 4/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0845 - mean_absolute_error: 0.1494\n",
      "Epoch 00004: val_loss improved from 0.05730 to 0.05375, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=32,c3=128,c4=0,filt1=7,filt2=5,filt3=7,filt4=0,str1=2,         str2=1,str3=1,str4=0, conv_double=False,globalpool=ave,dropout=0,dnodes=128,dropout=0.4,lr=0.002/weights.hdf5\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.0838 - mean_absolute_error: 0.1490 - val_loss: 0.0538 - val_mean_absolute_error: 0.1175\n",
      "Epoch 5/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0719 - mean_absolute_error: 0.1380\n",
      "Epoch 00005: val_loss improved from 0.05375 to 0.04983, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=32,c3=128,c4=0,filt1=7,filt2=5,filt3=7,filt4=0,str1=2,         str2=1,str3=1,str4=0, conv_double=False,globalpool=ave,dropout=0,dnodes=128,dropout=0.4,lr=0.002/weights.hdf5\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.0725 - mean_absolute_error: 0.1380 - val_loss: 0.0498 - val_mean_absolute_error: 0.1389\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0679 - mean_absolute_error: 0.1357\n",
      "Epoch 00006: val_loss did not improve from 0.04983\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.0679 - mean_absolute_error: 0.1357 - val_loss: 0.0544 - val_mean_absolute_error: 0.1145\n",
      "Epoch 7/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0645 - mean_absolute_error: 0.1326\n",
      "Epoch 00007: val_loss improved from 0.04983 to 0.04902, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=32,c3=128,c4=0,filt1=7,filt2=5,filt3=7,filt4=0,str1=2,         str2=1,str3=1,str4=0, conv_double=False,globalpool=ave,dropout=0,dnodes=128,dropout=0.4,lr=0.002/weights.hdf5\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.0641 - mean_absolute_error: 0.1327 - val_loss: 0.0490 - val_mean_absolute_error: 0.1381\n",
      "Epoch 8/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0646 - mean_absolute_error: 0.1335\n",
      "Epoch 00008: val_loss improved from 0.04902 to 0.04850, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=32,c3=128,c4=0,filt1=7,filt2=5,filt3=7,filt4=0,str1=2,         str2=1,str3=1,str4=0, conv_double=False,globalpool=ave,dropout=0,dnodes=128,dropout=0.4,lr=0.002/weights.hdf5\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.0633 - mean_absolute_error: 0.1331 - val_loss: 0.0485 - val_mean_absolute_error: 0.1285\n",
      "Epoch 9/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0642 - mean_absolute_error: 0.1321\n",
      "Epoch 00009: val_loss did not improve from 0.04850\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.0639 - mean_absolute_error: 0.1312 - val_loss: 0.0517 - val_mean_absolute_error: 0.1167\n",
      "Epoch 10/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0614 - mean_absolute_error: 0.1322\n",
      "Epoch 00010: val_loss improved from 0.04850 to 0.04666, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=32,c3=128,c4=0,filt1=7,filt2=5,filt3=7,filt4=0,str1=2,         str2=1,str3=1,str4=0, conv_double=False,globalpool=ave,dropout=0,dnodes=128,dropout=0.4,lr=0.002/weights.hdf5\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.0616 - mean_absolute_error: 0.1316 - val_loss: 0.0467 - val_mean_absolute_error: 0.1327\n",
      "Epoch 11/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0610 - mean_absolute_error: 0.1302\n",
      "Epoch 00011: val_loss did not improve from 0.04666\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.0616 - mean_absolute_error: 0.1300 - val_loss: 0.0475 - val_mean_absolute_error: 0.1281\n",
      "Epoch 12/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0607 - mean_absolute_error: 0.1302\n",
      "Epoch 00012: val_loss did not improve from 0.04666\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0612 - mean_absolute_error: 0.1302 - val_loss: 0.0470 - val_mean_absolute_error: 0.1364\n",
      "Epoch 13/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0597 - mean_absolute_error: 0.1302\n",
      "Epoch 00013: val_loss improved from 0.04666 to 0.04610, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=32,c2=32,c3=128,c4=0,filt1=7,filt2=5,filt3=7,filt4=0,str1=2,         str2=1,str3=1,str4=0, conv_double=False,globalpool=ave,dropout=0,dnodes=128,dropout=0.4,lr=0.002/weights.hdf5\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.0598 - mean_absolute_error: 0.1302 - val_loss: 0.0461 - val_mean_absolute_error: 0.1207\n",
      "Epoch 14/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0617 - mean_absolute_error: 0.1301\n",
      "Epoch 00014: val_loss did not improve from 0.04610\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.0607 - mean_absolute_error: 0.1299 - val_loss: 0.0561 - val_mean_absolute_error: 0.1122\n",
      "Epoch 15/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0627 - mean_absolute_error: 0.1305\n",
      "Epoch 00015: val_loss did not improve from 0.04610\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0623 - mean_absolute_error: 0.1313 - val_loss: 0.0469 - val_mean_absolute_error: 0.1239\n",
      "Epoch 16/100\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0606 - mean_absolute_error: 0.1282\n",
      "Epoch 00016: val_loss did not improve from 0.04610\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0607 - mean_absolute_error: 0.1284 - val_loss: 0.0468 - val_mean_absolute_error: 0.1311\n",
      "random search 2/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/painstudy/anaconda3/envs/keras/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3700 - mean_absolute_error: 0.6574\n",
      "Epoch 00001: val_loss improved from inf to 0.18297, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0_lr/batch=512,c1=64,c2=32,c3=64,c4=0,filt1=7,filt2=7,filt3=5,filt4=0,str1=1,         str2=2,str3=1,str4=0, conv_double=True,globalpool=max,dropout=0.5,dnodes=16,dropout=0.2,lr=0.002/weights.hdf5\n",
      "18/18 [==============================] - 4s 119ms/step - loss: 1.3700 - mean_absolute_error: 0.6574 - val_loss: 0.1830 - val_mean_absolute_error: 0.2171\n",
      "Epoch 2/100\n",
      " 3/18 [====>.........................] - ETA: 0s - loss: 0.8509 - mean_absolute_error: 0.5139"
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "\n",
    "\n",
    "# random search for hyperparameter\n",
    "ntrial = ntest\n",
    "train_errs, val_errs = [] ,[]\n",
    "#test_roc, test_prc = [], []\n",
    "test_rmse, test_mae, test_auc = [], [], []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    test_setting_1 = random.choice(test_settings_1)\n",
    "    test_setting_2 = random.choice(test_settings_2)\n",
    "    test_setting_3 = random.choice(test_settings_3)\n",
    "        \n",
    "        \n",
    "    # test_setting\n",
    "    num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4 = test_setting_1\n",
    "    dense_node, dropout_cnn, dropout_fc, globalpool_opt, batch_size, conv_double, learning_rate = test_setting_2\n",
    "    stride_l1, stride_l2, stride_l3, stride_l4, stride_l5, num_l5, kernel_l5 = test_setting_3\n",
    "    \n",
    "    \n",
    "    # total conv layers of the model\n",
    "    n_conv = random.choice([2,3,4])\n",
    "    \n",
    "    if n_conv==2:\n",
    "        num_l3,kernel_l3,stride_l3 = 0,0,0\n",
    "        num_l4,kernel_l4,stride_l4 = 0,0,0\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0\n",
    "    \n",
    "    if n_conv==3:\n",
    "        num_l4,kernel_l4,stride_l4 = 0,0,0\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0\n",
    "        \n",
    "    if n_conv==4:\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0    \n",
    "    \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = f'batch={batch_size},c1={num_l1},c2={num_l2},c3={num_l3},c4={num_l4},filt1={kernel_l1},filt2={kernel_l2},filt3={kernel_l3},filt4={kernel_l4},str1={stride_l1}, \\\n",
    "        str2={stride_l2},str3={stride_l3},str4={stride_l4}, conv_double={conv_double},globalpool={globalpool_opt},dropout={dropout_cnn},dnodes={dense_node},dropout={dropout_fc},lr={learning_rate}'\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "    weightcache = \"{}/weights.hdf5\".format(odir)        \n",
    "\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        # build a model\n",
    "        model = Sequential()\n",
    "\n",
    "        act='relu'\n",
    "\n",
    "        # c1 layer\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l1, kernel_size=kernel_l1, strides=stride_l1, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l1, kernel_size=kernel_l1, strides=stride_l1, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "\n",
    "        # c2 layer\n",
    "        if num_l1 == 512:\n",
    "            model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l2, kernel_size=kernel_l2, strides=stride_l2, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l2, kernel_size=kernel_l2, strides=stride_l2, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        \n",
    "        # c3 layer\n",
    "        if n_conv>2:\n",
    "            if num_l2 == 512:\n",
    "                model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "            if conv_double:\n",
    "                model.add(Conv1D(filters=num_l3, kernel_size=kernel_l3, strides=stride_l3, padding='same'))\n",
    "            model.add(Conv1D(filters=num_l3, kernel_size=kernel_l3, strides=stride_l3, padding='same', activation=act))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        \n",
    "        # c4 layer\n",
    "        if n_conv>3:\n",
    "            if num_l3 == 512:\n",
    "                model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "            if conv_double:\n",
    "                model.add(Conv1D(filters=num_l4, kernel_size=kernel_l4, strides=stride_l4, padding='same'))\n",
    "            model.add(Conv1D(filters=num_l4, kernel_size=kernel_l4, strides=stride_l4, padding='same', activation=act))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "            \n",
    "        # c5 layer\n",
    "        if n_conv>4:\n",
    "            if num_l4 == 512:\n",
    "                model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "            if conv_double:\n",
    "                model.add(Conv1D(filters=num_l5, kernel_size=kernel_l5, strides=stride_l5, padding='same'))\n",
    "            model.add(Conv1D(filters=num_l5, kernel_size=kernel_l5, strides=stride_l5,padding='same', activation=act))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))            \n",
    "            \n",
    "\n",
    "\n",
    "        # global이냐 flatten이냐는 따로 모델 나눠야 할듯\n",
    "        if globalpool_opt == 'max':\n",
    "            model.add(GlobalMaxPool1D())\n",
    "        elif globalpool_opt == 'ave':\n",
    "            model.add(GlobalAveragePooling1D())\n",
    "            \n",
    "            \n",
    "        if dense_node != 0:\n",
    "            model.add(Dropout(dropout_cnn))\n",
    "            model.add(Dense(dense_node, activation='tanh'))\n",
    "        model.add(Dropout(dropout_fc))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "\n",
    "        # model 학습 설정\n",
    "        try:\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=[\"mean_absolute_error\"])\n",
    "            hist = model.fit(x_train, y_train, sample_weight = train_w_samp, validation_data=(x_val, y_val, val_w_samp), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')])\n",
    "        except Exception as e:\n",
    "            print(f'error: {e}')\n",
    "            os.rmdir(odir)\n",
    "            #os.rename(odir,rootdir+'/error_{}'.format(odir_f))\n",
    "            itrial -= 1\n",
    "            test_mae.append(0)\n",
    "            test_rmse.append(0)\n",
    "            train_errs.append(-1)\n",
    "            val_errs.append(-1)\n",
    "            continue\n",
    "            \n",
    "\n",
    "    # 모델의 아키텍처 및 구조 저장\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    # test set에 대한 y_pred 계산\n",
    "    model.load_weights(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "    y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "    # auroc 계산\n",
    "    y_test_bin = y_test > 0\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_auc.append(roc_auc)\n",
    "    \n",
    "    # RMSE 계산\n",
    "    model_err = metrics.RootMeanSquaredError() \n",
    "    model_err.update_state(y_test, y_pred)\n",
    "    rmse_val = model_err.result().numpy()\n",
    "    test_rmse.append(rmse_val)\n",
    "    \n",
    "    # MAE 계산\n",
    "    model_err = metrics.MeanAbsoluteError()\n",
    "    model_err.update_state(y_test, y_pred)\n",
    "    mae_val = model_err.result().numpy()\n",
    "    test_mae.append(mae_val)\n",
    "    \n",
    "    \n",
    "    # acc 계산\n",
    "    #acc_val = np.mean((y_pred*10>=5)==y_test_bin)\n",
    "    #test_acc.append(acc_val)\n",
    "    \n",
    "    # rename\n",
    "    os.rename(odir, f'{rootdir}/roc{roc_auc:.3f}_mae{mae_val:.3f}_rmse{rmse_val:.3f}_{odir_f}')\n",
    "\n",
    "    # train 과정에서의 err\n",
    "    train_err = min(hist.history['loss'])\n",
    "    val_err = min(hist.history['val_loss'])\n",
    "\n",
    "    val_errs.append(val_err)\n",
    "    train_errs.append(train_err)\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "max_idx = test_roc.index(max(test_auc))\n",
    "print('\\nBest Model mae:{:.4f}, info: {}'.format(test_mae(max_idx), random_settings(max_idx)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c3d46-d870-4d7a-b41e-261f01d12e42",
   "metadata": {},
   "source": [
    "## Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966cf68f-6c01-4cd5-a424-3e8564c04378",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "        # model 학습 설정\n",
    "        try:\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=[\"acc\", tf.keras.metrics.AUC()])\n",
    "            hist = model.fit(x_train_ecg, y_train_bin, sample_weight=train_w_samp3, validation_data=(x_val_ecg, y_val_bin, val_w_samp3), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')])\n",
    "        except:\n",
    "            os.rmdir(odir)\n",
    "            #os.rename(odir,rootdir+'/error_{}'.format(odir_f))\n",
    "            itrial -= 1\n",
    "            test_roc.append(0)\n",
    "            test_acc.append(0)\n",
    "            test_prc.append(0)\n",
    "            train_errs.append(-1)\n",
    "            val_errs.append(-1)\n",
    "            continue\n",
    "            \n",
    "            \n",
    "    # 모델의 아키텍처 및 구조 저장\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    # test set에 대한 y_pred 계산\n",
    "    model.load_weights(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "    y_pred = model.predict(x_test_ecg).flatten()\n",
    "\n",
    "    \n",
    "    # acc 계산\n",
    "    acc = metrics.Accuracy()\n",
    "    acc.update_state(y_pred>=0.5, y_test_bin, sample_weight=test_w_samp3)\n",
    "    acc_val = acc.result().numpy()\n",
    "    test_acc.append(acc_val)\n",
    "    \n",
    "    # auroc 계산\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred, sample_weight=test_w_samp3)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_roc.append(roc_auc)\n",
    "\n",
    "    # auprc \n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin, y_pred, sample_weight=test_w_samp3)\n",
    "    prc_auc = auc(recall, precision)\n",
    "    test_prc.append(prc_auc)\n",
    "\n",
    "    \n",
    "    # rename\n",
    "    os.rename(odir, rootdir+'/roc{:.4f}_prc{:.4f}_{}_acc{:.2f}'.format(roc_auc, prc_auc, odir_f, acc_val))\n",
    "\n",
    "    # train 과정에서의 err\n",
    "    train_err = min(hist.history['loss'])\n",
    "    val_err = min(hist.history['val_loss'])\n",
    "\n",
    "    val_errs.append(val_err)\n",
    "    train_errs.append(train_err)\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "max_idx = test_roc.index(max(test_auc))\n",
    "print('\\nBest Model roc:{:.4f}, info: {}'.format(test_roc(max_idx), random_settings(max_idx)))\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "painstudy_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
