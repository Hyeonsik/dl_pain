{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c66fd7e-43ab-4acc-a260-5e086232268c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T13:55:36.008707Z",
     "iopub.status.busy": "2022-02-08T13:55:36.008095Z",
     "iopub.status.idle": "2022-02-08T13:55:38.238505Z",
     "shell.execute_reply": "2022-02-08T13:55:38.237941Z",
     "shell.execute_reply.started": "2022-02-08T13:55:36.008563Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import losses, metrics\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, BatchNormalization, Dropout, Activation\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, SeparableConv1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "import tensorflow as tf\n",
    "import os, sys, pickle\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afdbb9-8128-4e8f-8dcc-55e5bd2184fe",
   "metadata": {},
   "source": [
    "# GPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f5990a-1951-44b6-ba7f-4cb6c9e1ff2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T13:55:41.231365Z",
     "iopub.status.busy": "2022-02-08T13:55:41.230873Z",
     "iopub.status.idle": "2022-02-08T13:55:41.236298Z",
     "shell.execute_reply": "2022-02-08T13:55:41.235391Z",
     "shell.execute_reply.started": "2022-02-08T13:55:41.231315Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1741e5-8d15-439f-8c98-e5584d241cd3",
   "metadata": {},
   "source": [
    "# 1. Loading input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0b0c42-49c7-400b-a249-cd30cb4e90a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T13:55:42.694285Z",
     "iopub.status.busy": "2022-02-08T13:55:42.693748Z",
     "iopub.status.idle": "2022-02-08T13:55:44.008304Z",
     "shell.execute_reply": "2022-02-08T13:55:44.007815Z",
     "shell.execute_reply.started": "2022-02-08T13:55:42.694232Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train...done\n",
      "x_train shape: (9135, 2000, 2)\n",
      "x_test.shape: (1159, 2000, 2)\n",
      "x_val.shape: (1023, 2000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "MAX_CASES = 2000\n",
    "SEGLEN_IN_SEC = 20\n",
    "SRATE = 100\n",
    "LEN_INPUT = 20\n",
    "OVERLAP = 10\n",
    "LEN_PER_PRE = 60\n",
    "LEN_PER_POST = 60\n",
    "\n",
    "\n",
    "print('loading train...', flush=True, end='')\n",
    "\n",
    "# x를 loading해서 (batch_size, step, channel)\n",
    "input_path = f\"../DL_model/dataset/ne{LEN_PER_PRE}s-e{LEN_PER_POST}s-len{LEN_INPUT}-{OVERLAP}/\"\n",
    "x_train = np.load(input_path+'x_train.npz', allow_pickle=True)['arr_0']\n",
    "x_test = np.load(input_path+'x_test.npz', allow_pickle=True)['arr_0']\n",
    "x_val = np.load(input_path+'x_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "tss_train = np.load(input_path+'tss_train.npz')['arr_0']\n",
    "tss_test = np.load(input_path+'tss_test.npz')['arr_0']\n",
    "tss_val = np.load(input_path+'tss_val.npz')['arr_0']\n",
    "cisa_train = np.load(input_path+'cisa_train.npz')['arr_0']\n",
    "cisa_test = np.load(input_path+'cisa_test.npz')['arr_0']\n",
    "cisa_val = np.load(input_path+'cisa_val.npz')['arr_0']\n",
    "\n",
    "gender_train = np.load(input_path+'gender_train.npz', allow_pickle=True)['arr_0']\n",
    "gender_test = np.load(input_path+'gender_test.npz', allow_pickle=True)['arr_0']\n",
    "gender_val = np.load(input_path+'gender_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "age_train = np.load(input_path+'age_train.npz', allow_pickle=True)['arr_0']\n",
    "age_test = np.load(input_path+'age_test.npz', allow_pickle=True)['arr_0']\n",
    "age_val = np.load(input_path+'age_val.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "\n",
    "print('done', flush=True)\n",
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test.shape:', x_test.shape)\n",
    "print('x_val.shape:', x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98bc2f-bdf8-4400-9f0d-4612c232c2d1",
   "metadata": {},
   "source": [
    "## Input, output settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b8f66d5-d6dd-4ec1-922b-1dc634dc3be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T13:55:44.694936Z",
     "iopub.status.busy": "2022-02-08T13:55:44.694536Z",
     "iopub.status.idle": "2022-02-08T13:55:44.700604Z",
     "shell.execute_reply": "2022-02-08T13:55:44.699703Z",
     "shell.execute_reply.started": "2022-02-08T13:55:44.694900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input mode\n",
    "mode_in = 'PPG+ECG'\n",
    "\n",
    "if mode_in == 'PPG':\n",
    "    x_train = x_train[:,:,0:1]\n",
    "    x_test = x_test[:,:,0:1]\n",
    "    x_val = x_val[:,:,0:1]\n",
    "elif mode_in == 'ECG':\n",
    "    x_train = x_train[:,:,1:2]\n",
    "    x_test = x_test[:,:,1:2]\n",
    "    x_val = x_val[:,:,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfcdfab7-934b-4686-b59e-11ab0108a447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T13:55:46.420740Z",
     "iopub.status.busy": "2022-02-08T13:55:46.420226Z",
     "iopub.status.idle": "2022-02-08T13:55:46.426762Z",
     "shell.execute_reply": "2022-02-08T13:55:46.425680Z",
     "shell.execute_reply.started": "2022-02-08T13:55:46.420691Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = 'TSS'\n",
    "\n",
    "if mode == 'TSS':\n",
    "    # tss를 output으로 할 경우\n",
    "    y_train = tss_train\n",
    "    y_val = tss_val\n",
    "    y_test = tss_test\n",
    "\n",
    "elif mode == 'CISA':\n",
    "    # cisa를 output으로 할 경우\n",
    "    y_train = cisa_train\n",
    "    y_val = cisa_val\n",
    "    y_test = cisa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da180113-aec2-4894-af6d-115036200d93",
   "metadata": {},
   "source": [
    "## Sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4838805-26d2-4ef8-94f3-c38da8a24148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T16:01:28.595800Z",
     "iopub.status.busy": "2022-02-08T16:01:28.595191Z",
     "iopub.status.idle": "2022-02-08T16:01:28.621250Z",
     "shell.execute_reply": "2022-02-08T16:01:28.620395Z",
     "shell.execute_reply.started": "2022-02-08T16:01:28.595742Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample weight for no pain: 3.36, moderate pain: 1.42\n",
      "sample weight for no pain: 4.44, severe pain: 1.29\n",
      "sample weight for no pain: 2.94, moderate pain: 1.52\n",
      "sample weight for no pain: 3.69, severe pain: 1.37\n",
      "sample weight for no pain: 3.34, moderate pain: 1.43\n",
      "sample weight for no pain: 4.60, severe pain: 1.28\n"
     ]
    }
   ],
   "source": [
    "# 2 class에 대한 sample weight\n",
    "y_train_bin = y_train > 0\n",
    "y_val_bin = y_val > 0\n",
    "y_test_bin = y_test > 0\n",
    "\n",
    "y_train_bin2 = y_train > 0.2\n",
    "y_val_bin2 = y_val > 0.2\n",
    "y_test_bin2 = y_test > 0.2\n",
    "\n",
    "\n",
    "train_w_samp2 = np.ones(shape=(len(y_train),))\n",
    "train_w_samp2[y_train_bin==0]= len(y_train) / np.sum(y_train_bin)\n",
    "train_w_samp2[y_train_bin!=0]= len(y_train) / np.sum(~y_train_bin)\n",
    "\n",
    "train_w_samp2_2 = np.ones(shape=(len(y_train),))\n",
    "train_w_samp2_2[y_train_bin==0]= len(y_train) / np.sum(y_train_bin2)\n",
    "train_w_samp2_2[y_train_bin!=0]= len(y_train) / np.sum(~y_train_bin2)\n",
    "\n",
    "print('sample weight for no pain: {:.2f}, moderate pain: {:.2f}'\n",
    "      .format(len(y_train) / np.sum(y_train_bin), len(y_train) / np.sum(~y_train_bin)))\n",
    "print('sample weight for no pain: {:.2f}, severe pain: {:.2f}'\n",
    "      .format(len(y_train) / np.sum(y_train_bin2), len(y_train) / np.sum(~y_train_bin2)))\n",
    "\n",
    "# 2 class에 대한 sample weight\n",
    "val_w_samp2 = np.ones(shape=(len(y_val),))\n",
    "val_w_samp2[y_val_bin==0]= len(y_val) / np.sum(y_val_bin)\n",
    "val_w_samp2[y_val_bin!=0]= len(y_val) / np.sum(~y_val_bin)\n",
    "\n",
    "val_w_samp2_2 = np.ones(shape=(len(y_val),))\n",
    "val_w_samp2_2[y_val_bin==0]= len(y_val) / np.sum(y_val_bin2)\n",
    "val_w_samp2_2[y_val_bin!=0]= len(y_val) / np.sum(~y_val_bin2)\n",
    "\n",
    "print('sample weight for no pain: {:.2f}, moderate pain: {:.2f}'\n",
    "      .format(len(y_val) / np.sum(y_val_bin), len(y_val) / np.sum(~y_val_bin)))\n",
    "print('sample weight for no pain: {:.2f}, severe pain: {:.2f}'\n",
    "      .format(len(y_val) / np.sum(y_val_bin2), len(y_val) / np.sum(~y_val_bin2)))\n",
    "\n",
    "# 2 class에 대한 sample weight\n",
    "test_w_samp2 = np.ones(shape=(len(y_test),))\n",
    "test_w_samp2[y_test_bin==0]= len(y_test) / np.sum(y_test_bin)\n",
    "test_w_samp2[y_test_bin!=0]= len(y_test) / np.sum(~y_test_bin)\n",
    "\n",
    "test_w_samp2_2 = np.ones(shape=(len(y_test),))\n",
    "test_w_samp2_2[y_test_bin==0]= len(y_test) / np.sum(y_test_bin2)\n",
    "test_w_samp2_2[y_test_bin!=0]= len(y_test) / np.sum(~y_test_bin2)\n",
    "\n",
    "print('sample weight for no pain: {:.2f}, moderate pain: {:.2f}'\n",
    "      .format(len(y_test) / np.sum(y_test_bin), len(y_test) / np.sum(~y_test_bin)))\n",
    "print('sample weight for no pain: {:.2f}, severe pain: {:.2f}'\n",
    "      .format(len(y_test) / np.sum(y_test_bin2), len(y_test) / np.sum(~y_test_bin2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49391ffd-97e8-4f8f-a5a6-dcf8bcb6347e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T16:22:24.409972Z",
     "iopub.status.busy": "2022-02-08T16:22:24.409423Z",
     "iopub.status.idle": "2022-02-08T16:22:24.415508Z",
     "shell.execute_reply": "2022-02-08T16:22:24.414296Z",
     "shell.execute_reply.started": "2022-02-08T16:22:24.409919Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_w_samp = train_w_samp2_2\n",
    "val_w_samp = val_w_samp2_2\n",
    "test_w_samp = test_w_samp2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e8845-7d15-4713-8b3e-5a4dfb6ed4de",
   "metadata": {},
   "source": [
    "# 2. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a5847ab-4bb4-47ea-a32a-1761437e121d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T16:21:34.802634Z",
     "iopub.status.busy": "2022-02-08T16:21:34.802058Z",
     "iopub.status.idle": "2022-02-08T16:21:35.739513Z",
     "shell.execute_reply": "2022-02-08T16:21:35.738820Z",
     "shell.execute_reply.started": "2022-02-08T16:21:34.802579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making test settings...done\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "nfold = 1  # 각각의 hyperparameter에 대해 k-fold 를 시행하고 평균을 구한다.\n",
    "ntest = 500\n",
    "rootdir = f\"randomSearch/TSS/CNN_{mode_in}_4layers_Reg_{nfold}fold_test{ntest}_w_samp0.2\"\n",
    "\n",
    "if not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "# 모델에 대한 정보 txt로 저장\n",
    "f = open(f'{rootdir}/README.txt', 'w')\n",
    "f.write(f'model: 1D CNN 4 layers, regression')\n",
    "f.write(f'input: {mode_in} (pre-intubation 120~60s, post-intubation 60~120s), output: {mode}')\n",
    "f.close()\n",
    "    \n",
    "\n",
    "# test_settings\n",
    "test_settings_1, test_settings_2, test_settings_3 = [], [], []\n",
    "\n",
    "\n",
    "# hyperparamters\n",
    "#num_nodes = [64, 64, 64] #, 64, 64, 64]\n",
    "#kernel_size = 10\n",
    "pool_size = 2\n",
    "\n",
    "#dense_node = 32\n",
    "#dropout_rate = 0.2\n",
    "learning_rate = 0.002\n",
    "\n",
    "# hyperparamters pool\n",
    "num_opts = [32, 64, 128, 256] # num of filters(kernel)\n",
    "stride_opts = [1,1,1,1,1,2,2,2,2]\n",
    "kernel_opts = range(3,9,2) # kernel size\n",
    "dropout_opts  = [0, 0.1, 0.2, 0.3, 0.4, 0.5] # dropout rate\n",
    "dense_opts = [0, 8, 16, 32, 64]\n",
    "globalpool_opts = ['max','ave']\n",
    "BATCH_SIZE = [512, 1024]\n",
    "\n",
    "\n",
    "print('start making test settings...', end='', flush=True)\n",
    "# test settings\n",
    "for num_l1 in num_opts:\n",
    "    for num_l2 in num_opts:\n",
    "        for num_l3 in num_opts:\n",
    "            for num_l4 in num_opts:\n",
    "                for kernel_l1 in kernel_opts:\n",
    "                    for kernel_l2 in kernel_opts:\n",
    "                        for kernel_l3 in kernel_opts:\n",
    "                            for kernel_l4 in kernel_opts:\n",
    "                                test_settings_1.append([num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4])\n",
    "\n",
    "for dense_node in dense_opts:\n",
    "    for dropout_cnn in dropout_opts:\n",
    "        for dropout_fc in dropout_opts:\n",
    "            for globalpool_opt in globalpool_opts:\n",
    "                for batch_size in BATCH_SIZE:\n",
    "                    for conv_double in [True, False]:\n",
    "                        test_settings_2.append([dense_node, dropout_cnn, dropout_fc, globalpool_opt, batch_size, conv_double])                                   \n",
    "\n",
    "for stride_l1 in stride_opts:\n",
    "    for stride_l2 in stride_opts:\n",
    "        for stride_l3 in stride_opts:\n",
    "            for stride_l4 in stride_opts:\n",
    "                for stride_l5 in stride_opts:\n",
    "                    for num_l5 in num_opts:\n",
    "                        for kernel_l5 in kernel_opts:\n",
    "                            test_settings_3.append([stride_l1, stride_l2, stride_l3, stride_l4, stride_l5, num_l5, kernel_l5])\n",
    "                        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f7135-75f1-4a1b-9860-18fb0a9333f7",
   "metadata": {},
   "source": [
    "# 3. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0de30-e0ce-49c8-83a9-3d512d5e438a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058a1aa-0441-41b0-82cf-f4198f0e3c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T16:22:31.684817Z",
     "iopub.status.busy": "2022-02-08T16:22:31.684252Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random search 0/500\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.6376 - mean_absolute_error: 0.6499\n",
      "Epoch 00001: val_loss improved from inf to 0.28186, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 3s 81ms/step - loss: 2.6376 - mean_absolute_error: 0.6499 - val_loss: 0.2819 - val_mean_absolute_error: 0.2569\n",
      "Epoch 2/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.7261 - mean_absolute_error: 0.3711\n",
      "Epoch 00002: val_loss did not improve from 0.28186\n",
      "18/18 [==============================] - 1s 54ms/step - loss: 0.7191 - mean_absolute_error: 0.3695 - val_loss: 0.3347 - val_mean_absolute_error: 0.2780\n",
      "Epoch 3/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.4306 - mean_absolute_error: 0.2929\n",
      "Epoch 00003: val_loss improved from 0.28186 to 0.12261, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.4283 - mean_absolute_error: 0.2924 - val_loss: 0.1226 - val_mean_absolute_error: 0.1731\n",
      "Epoch 4/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.3232 - mean_absolute_error: 0.2536\n",
      "Epoch 00004: val_loss improved from 0.12261 to 0.08849, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.3212 - mean_absolute_error: 0.2529 - val_loss: 0.0885 - val_mean_absolute_error: 0.1481\n",
      "Epoch 5/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2575 - mean_absolute_error: 0.2290\n",
      "Epoch 00005: val_loss improved from 0.08849 to 0.05143, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.2558 - mean_absolute_error: 0.2286 - val_loss: 0.0514 - val_mean_absolute_error: 0.1197\n",
      "Epoch 6/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2023 - mean_absolute_error: 0.2041\n",
      "Epoch 00006: val_loss did not improve from 0.05143\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.2011 - mean_absolute_error: 0.2043 - val_loss: 0.0672 - val_mean_absolute_error: 0.1379\n",
      "Epoch 7/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1576 - mean_absolute_error: 0.1838\n",
      "Epoch 00007: val_loss improved from 0.05143 to 0.05087, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.1568 - mean_absolute_error: 0.1835 - val_loss: 0.0509 - val_mean_absolute_error: 0.1217\n",
      "Epoch 8/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1355 - mean_absolute_error: 0.1715\n",
      "Epoch 00008: val_loss improved from 0.05087 to 0.04317, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 0.1342 - mean_absolute_error: 0.1704 - val_loss: 0.0432 - val_mean_absolute_error: 0.1132\n",
      "Epoch 9/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1097 - mean_absolute_error: 0.1547\n",
      "Epoch 00009: val_loss improved from 0.04317 to 0.03958, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 0.1086 - mean_absolute_error: 0.1544 - val_loss: 0.0396 - val_mean_absolute_error: 0.1034\n",
      "Epoch 10/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0952 - mean_absolute_error: 0.1450\n",
      "Epoch 00010: val_loss improved from 0.03958 to 0.03745, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 59ms/step - loss: 0.0945 - mean_absolute_error: 0.1445 - val_loss: 0.0374 - val_mean_absolute_error: 0.1054\n",
      "Epoch 11/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0811 - mean_absolute_error: 0.1349\n",
      "Epoch 00011: val_loss improved from 0.03745 to 0.03508, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.0806 - mean_absolute_error: 0.1344 - val_loss: 0.0351 - val_mean_absolute_error: 0.1044\n",
      "Epoch 12/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0676 - mean_absolute_error: 0.1248\n",
      "Epoch 00012: val_loss improved from 0.03508 to 0.03340, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.0675 - mean_absolute_error: 0.1244 - val_loss: 0.0334 - val_mean_absolute_error: 0.0988\n",
      "Epoch 13/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0625 - mean_absolute_error: 0.1176\n",
      "Epoch 00013: val_loss improved from 0.03340 to 0.03283, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.0627 - mean_absolute_error: 0.1183 - val_loss: 0.0328 - val_mean_absolute_error: 0.1006\n",
      "Epoch 14/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0547 - mean_absolute_error: 0.1124\n",
      "Epoch 00014: val_loss improved from 0.03283 to 0.03248, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.0548 - mean_absolute_error: 0.1124 - val_loss: 0.0325 - val_mean_absolute_error: 0.0966\n",
      "Epoch 15/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0506 - mean_absolute_error: 0.1081\n",
      "Epoch 00015: val_loss improved from 0.03248 to 0.03244, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=64,c2=32,c3=128,c4=128,filt1=5,filt2=5,filt3=5,filt4=5,str1=1,str2=2,str3=2,str4=2, conv_double=False,globalpool=max,dropout=0.5,dnodes=64,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.0499 - mean_absolute_error: 0.1073 - val_loss: 0.0324 - val_mean_absolute_error: 0.0974\n",
      "Epoch 16/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0493 - mean_absolute_error: 0.1057\n",
      "Epoch 00016: val_loss did not improve from 0.03244\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.0490 - mean_absolute_error: 0.1051 - val_loss: 0.0329 - val_mean_absolute_error: 0.0959\n",
      "Epoch 17/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0452 - mean_absolute_error: 0.1024\n",
      "Epoch 00017: val_loss did not improve from 0.03244\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.0452 - mean_absolute_error: 0.1023 - val_loss: 0.0330 - val_mean_absolute_error: 0.0981\n",
      "Epoch 18/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.0437 - mean_absolute_error: 0.1002\n",
      "Epoch 00018: val_loss did not improve from 0.03244\n",
      "18/18 [==============================] - 1s 56ms/step - loss: 0.0437 - mean_absolute_error: 0.1004 - val_loss: 0.0331 - val_mean_absolute_error: 0.0961\n",
      "random search 1/500\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.2150 - mean_absolute_error: 0.1984\n",
      "Epoch 00001: val_loss improved from inf to 0.13751, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=256,c2=32,c3=32,c4=0,filt1=5,filt2=7,filt3=3,filt4=0,str1=1,str2=1,str3=1,str4=0, conv_double=False,globalpool=ave,dropout=0.2,dnodes=16,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 4s 144ms/step - loss: 0.2150 - mean_absolute_error: 0.1984 - val_loss: 0.1375 - val_mean_absolute_error: 0.1800\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0789 - mean_absolute_error: 0.1352\n",
      "Epoch 00002: val_loss improved from 0.13751 to 0.03920, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=256,c2=32,c3=32,c4=0,filt1=5,filt2=7,filt3=3,filt4=0,str1=1,str2=1,str3=1,str4=0, conv_double=False,globalpool=ave,dropout=0.2,dnodes=16,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.0789 - mean_absolute_error: 0.1352 - val_loss: 0.0392 - val_mean_absolute_error: 0.1120\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0591 - mean_absolute_error: 0.1202\n",
      "Epoch 00003: val_loss improved from 0.03920 to 0.03259, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=512,c1=256,c2=32,c3=32,c4=0,filt1=5,filt2=7,filt3=3,filt4=0,str1=1,str2=1,str3=1,str4=0, conv_double=False,globalpool=ave,dropout=0.2,dnodes=16,dropout=0/weights.hdf5\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.0591 - mean_absolute_error: 0.1202 - val_loss: 0.0326 - val_mean_absolute_error: 0.1012\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0522 - mean_absolute_error: 0.1133\n",
      "Epoch 00004: val_loss did not improve from 0.03259\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 0.0522 - mean_absolute_error: 0.1133 - val_loss: 0.0370 - val_mean_absolute_error: 0.1124\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0479 - mean_absolute_error: 0.1090\n",
      "Epoch 00005: val_loss did not improve from 0.03259\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 0.0479 - mean_absolute_error: 0.1090 - val_loss: 0.0384 - val_mean_absolute_error: 0.1146\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.1045\n",
      "Epoch 00006: val_loss did not improve from 0.03259\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 0.0442 - mean_absolute_error: 0.1045 - val_loss: 0.0398 - val_mean_absolute_error: 0.1169\n",
      "random search 2/500\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 3.0763 - mean_absolute_error: 0.7453\n",
      "Epoch 00001: val_loss improved from inf to 0.62495, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=1024,c1=32,c2=64,c3=0,c4=0,filt1=5,filt2=5,filt3=0,filt4=0,str1=2,str2=1,str3=0,str4=0, conv_double=True,globalpool=max,dropout=0,dnodes=32,dropout=0.3/weights.hdf5\n",
      "9/9 [==============================] - 2s 105ms/step - loss: 3.0763 - mean_absolute_error: 0.7453 - val_loss: 0.6250 - val_mean_absolute_error: 0.3597\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.8330 - mean_absolute_error: 0.5849\n",
      "Epoch 00002: val_loss did not improve from 0.62495\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 1.8330 - mean_absolute_error: 0.5849 - val_loss: 1.1697 - val_mean_absolute_error: 0.5162\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.3315 - mean_absolute_error: 0.5008\n",
      "Epoch 00003: val_loss did not improve from 0.62495\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 1.3315 - mean_absolute_error: 0.5008 - val_loss: 1.4034 - val_mean_absolute_error: 0.6034\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.9402 - mean_absolute_error: 0.4181\n",
      "Epoch 00004: val_loss improved from 0.62495 to 0.28232, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=1024,c1=32,c2=64,c3=0,c4=0,filt1=5,filt2=5,filt3=0,filt4=0,str1=2,str2=1,str3=0,str4=0, conv_double=True,globalpool=max,dropout=0,dnodes=32,dropout=0.3/weights.hdf5\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.9402 - mean_absolute_error: 0.4181 - val_loss: 0.2823 - val_mean_absolute_error: 0.2496\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5970 - mean_absolute_error: 0.3365\n",
      "Epoch 00005: val_loss did not improve from 0.28232\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.5970 - mean_absolute_error: 0.3365 - val_loss: 0.3105 - val_mean_absolute_error: 0.2578\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.3903 - mean_absolute_error: 0.2739\n",
      "Epoch 00006: val_loss did not improve from 0.28232\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.3903 - mean_absolute_error: 0.2739 - val_loss: 0.9541 - val_mean_absolute_error: 0.5076\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.2585 - mean_absolute_error: 0.2274\n",
      "Epoch 00007: val_loss did not improve from 0.28232\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2585 - mean_absolute_error: 0.2274 - val_loss: 0.2969 - val_mean_absolute_error: 0.2622\n",
      "random search 3/500\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 5.0004 - mean_absolute_error: 0.8681\n",
      "Epoch 00001: val_loss improved from inf to 4.56803, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=1024,c1=256,c2=128,c3=0,c4=0,filt1=3,filt2=5,filt3=0,filt4=0,str1=2,str2=2,str3=0,str4=0, conv_double=True,globalpool=ave,dropout=0.2,dnodes=64,dropout=0.4/weights.hdf5\n",
      "9/9 [==============================] - 4s 259ms/step - loss: 5.0004 - mean_absolute_error: 0.8681 - val_loss: 4.5680 - val_mean_absolute_error: 1.2713\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.7618 - mean_absolute_error: 0.5700\n",
      "Epoch 00002: val_loss improved from 4.56803 to 0.50542, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=1024,c1=256,c2=128,c3=0,c4=0,filt1=3,filt2=5,filt3=0,filt4=0,str1=2,str2=2,str3=0,str4=0, conv_double=True,globalpool=ave,dropout=0.2,dnodes=64,dropout=0.4/weights.hdf5\n",
      "9/9 [==============================] - 2s 180ms/step - loss: 1.7618 - mean_absolute_error: 0.5700 - val_loss: 0.5054 - val_mean_absolute_error: 0.3217\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.3279 - mean_absolute_error: 0.4908\n",
      "Epoch 00003: val_loss improved from 0.50542 to 0.27731, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=1024,c1=256,c2=128,c3=0,c4=0,filt1=3,filt2=5,filt3=0,filt4=0,str1=2,str2=2,str3=0,str4=0, conv_double=True,globalpool=ave,dropout=0.2,dnodes=64,dropout=0.4/weights.hdf5\n",
      "9/9 [==============================] - 2s 182ms/step - loss: 1.3279 - mean_absolute_error: 0.4908 - val_loss: 0.2773 - val_mean_absolute_error: 0.2489\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.9421 - mean_absolute_error: 0.4175\n",
      "Epoch 00004: val_loss did not improve from 0.27731\n",
      "9/9 [==============================] - 2s 173ms/step - loss: 0.9421 - mean_absolute_error: 0.4175 - val_loss: 0.2969 - val_mean_absolute_error: 0.2782\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7893 - mean_absolute_error: 0.3782\n",
      "Epoch 00005: val_loss did not improve from 0.27731\n",
      "9/9 [==============================] - 2s 173ms/step - loss: 0.7893 - mean_absolute_error: 0.3782 - val_loss: 0.3921 - val_mean_absolute_error: 0.2996\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6627 - mean_absolute_error: 0.3496\n",
      "Epoch 00006: val_loss improved from 0.27731 to 0.06819, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=1024,c1=256,c2=128,c3=0,c4=0,filt1=3,filt2=5,filt3=0,filt4=0,str1=2,str2=2,str3=0,str4=0, conv_double=True,globalpool=ave,dropout=0.2,dnodes=64,dropout=0.4/weights.hdf5\n",
      "9/9 [==============================] - 2s 182ms/step - loss: 0.6627 - mean_absolute_error: 0.3496 - val_loss: 0.0682 - val_mean_absolute_error: 0.1454\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5820 - mean_absolute_error: 0.3293\n",
      "Epoch 00007: val_loss improved from 0.06819 to 0.04828, saving model to randomSearch/TSS/CNN_PPG+ECG_4layers_Reg_1fold_test500_w_samp0.2/batch=1024,c1=256,c2=128,c3=0,c4=0,filt1=3,filt2=5,filt3=0,filt4=0,str1=2,str2=2,str3=0,str4=0, conv_double=True,globalpool=ave,dropout=0.2,dnodes=64,dropout=0.4/weights.hdf5\n",
      "9/9 [==============================] - 2s 181ms/step - loss: 0.5820 - mean_absolute_error: 0.3293 - val_loss: 0.0483 - val_mean_absolute_error: 0.1201\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5180 - mean_absolute_error: 0.3132\n",
      "Epoch 00008: val_loss did not improve from 0.04828\n",
      "9/9 [==============================] - 2s 174ms/step - loss: 0.5180 - mean_absolute_error: 0.3132 - val_loss: 0.0537 - val_mean_absolute_error: 0.1191\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.4800 - mean_absolute_error: 0.2996\n",
      "Epoch 00009: val_loss did not improve from 0.04828\n",
      "9/9 [==============================] - 2s 173ms/step - loss: 0.4800 - mean_absolute_error: 0.2996 - val_loss: 0.1083 - val_mean_absolute_error: 0.1986\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.4407 - mean_absolute_error: 0.2901\n",
      "Epoch 00010: val_loss did not improve from 0.04828\n",
      "9/9 [==============================] - 2s 175ms/step - loss: 0.4407 - mean_absolute_error: 0.2901 - val_loss: 0.0518 - val_mean_absolute_error: 0.1243\n",
      "random search 4/500\n",
      "Epoch 1/100\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 1.4924 - mean_absolute_error: 0.5155"
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "\n",
    "\n",
    "# random search for hyperparameter\n",
    "ntrial = ntest\n",
    "train_errs, val_errs = [] ,[]\n",
    "#test_roc, test_prc = [], []\n",
    "test_rmse, test_mae = [], []\n",
    "random_settings = []\n",
    "\n",
    "\n",
    "for itrial in range(ntrial):\n",
    "    # grid search\n",
    "    # test_setting = test_settings[itrial]\n",
    "\n",
    "    # random search\n",
    "    print('random search {}/{}'.format(itrial, ntrial))\n",
    "    test_setting_1 = random.choice(test_settings_1)\n",
    "    test_setting_2 = random.choice(test_settings_2)\n",
    "    test_setting_3 = random.choice(test_settings_3)\n",
    "        \n",
    "        \n",
    "    # test_setting\n",
    "    num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4 = test_setting_1\n",
    "    dense_node, dropout_cnn, dropout_fc, globalpool_opt, batch_size, conv_double = test_setting_2\n",
    "    stride_l1, stride_l2, stride_l3, stride_l4, stride_l5, num_l5, kernel_l5 = test_setting_3\n",
    "    \n",
    "    \n",
    "    # total conv layers of the model\n",
    "    n_conv = random.choice([2,3,4])\n",
    "    \n",
    "    if n_conv==2:\n",
    "        num_l3,kernel_l3,stride_l3 = 0,0,0\n",
    "        num_l4,kernel_l4,stride_l4 = 0,0,0\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0\n",
    "    \n",
    "    if n_conv==3:\n",
    "        num_l4,kernel_l4,stride_l4 = 0,0,0\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0\n",
    "        \n",
    "    if n_conv==4:\n",
    "        num_l5,kernel_l5,stride_l5 = 0,0,0    \n",
    "    \n",
    "\n",
    "    # 이번 옵션에 대한 결과 디렉토리\n",
    "    odir_f = 'batch={},c1={},c2={},c3={},c4={},filt1={},filt2={},filt3={},filt4={},str1={},str2={},str3={},str4={}, conv_double={},globalpool={},dropout={},dnodes={},dropout={}'.format(batch_size, num_l1, num_l2, num_l3, num_l4, kernel_l1, kernel_l2, kernel_l3, kernel_l4,stride_l1,stride_l2,stride_l3,stride_l4,conv_double, globalpool_opt, dropout_cnn, dense_node, dropout_fc)\n",
    "    random_settings.append(odir_f)\n",
    "    \n",
    "    odir = rootdir + '/' + odir_f\n",
    "    if not os.path.exists(odir):\n",
    "        os.mkdir(odir)\n",
    "\n",
    "    weightcache = \"{}/weights.hdf5\".format(odir)        \n",
    "\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        # build a model\n",
    "        model = Sequential()\n",
    "\n",
    "        act='relu'\n",
    "\n",
    "        # c1 layer\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l1, kernel_size=kernel_l1, strides=stride_l1, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l1, kernel_size=kernel_l1, strides=stride_l1, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "\n",
    "        # c2 layer\n",
    "        if num_l1 == 512:\n",
    "            model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "        if conv_double:\n",
    "            model.add(Conv1D(filters=num_l2, kernel_size=kernel_l2, strides=stride_l2, padding='same'))\n",
    "        model.add(Conv1D(filters=num_l2, kernel_size=kernel_l2, strides=stride_l2, padding='same', activation=act))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        \n",
    "        # c3 layer\n",
    "        if n_conv>2:\n",
    "            if num_l2 == 512:\n",
    "                model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "            if conv_double:\n",
    "                model.add(Conv1D(filters=num_l3, kernel_size=kernel_l3, strides=stride_l3, padding='same'))\n",
    "            model.add(Conv1D(filters=num_l3, kernel_size=kernel_l3, strides=stride_l3, padding='same', activation=act))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        \n",
    "        # c4 layer\n",
    "        if n_conv>3:\n",
    "            if num_l3 == 512:\n",
    "                model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "            if conv_double:\n",
    "                model.add(Conv1D(filters=num_l4, kernel_size=kernel_l4, strides=stride_l4, padding='same'))\n",
    "            model.add(Conv1D(filters=num_l4, kernel_size=kernel_l4, strides=stride_l4, padding='same', activation=act))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "            \n",
    "        # c5 layer\n",
    "        if n_conv>4:\n",
    "            if num_l4 == 512:\n",
    "                model.add(Conv1D(filters=128,kernel_size=1,padding='same'))\n",
    "            if conv_double:\n",
    "                model.add(Conv1D(filters=num_l5, kernel_size=kernel_l5, strides=stride_l5, padding='same'))\n",
    "            model.add(Conv1D(filters=num_l5, kernel_size=kernel_l5, strides=stride_l5,padding='same', activation=act))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))            \n",
    "            \n",
    "\n",
    "\n",
    "        # global이냐 flatten이냐는 따로 모델 나눠야 할듯\n",
    "        if globalpool_opt == 'max':\n",
    "            model.add(GlobalMaxPool1D())\n",
    "        elif globalpool_opt == 'ave':\n",
    "            model.add(GlobalAveragePooling1D())\n",
    "            \n",
    "            \n",
    "        if dense_node != 0:\n",
    "            model.add(Dropout(dropout_cnn))\n",
    "            model.add(Dense(dense_node, activation='tanh'))\n",
    "        model.add(Dropout(dropout_fc))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "\n",
    "        # model 학습 설정\n",
    "        try:\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=[\"mean_absolute_error\"])\n",
    "            hist = model.fit(x_train, y_train, sample_weight = train_w_samp, validation_data=(x_val, y_val, val_w_samp), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')])\n",
    "        except Exception as e:\n",
    "            print(f'error: {e}')\n",
    "            os.rmdir(odir)\n",
    "            #os.rename(odir,rootdir+'/error_{}'.format(odir_f))\n",
    "            itrial -= 1\n",
    "            test_mae.append(0)\n",
    "            test_rmse.append(0)\n",
    "            train_errs.append(-1)\n",
    "            val_errs.append(-1)\n",
    "            continue\n",
    "            \n",
    "\n",
    "    # 모델의 아키텍처 및 구조 저장\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    # test set에 대한 y_pred 계산\n",
    "    model.load_weights(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "    y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "    # auroc 계산\n",
    "    #false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred)\n",
    "    #roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    #test_auc.append(roc_auc)\n",
    "    \n",
    "    # RMSE 계산\n",
    "    model_err = metrics.RootMeanSquaredError()\n",
    "    model_err.update_state(y_test, y_pred)\n",
    "    rmse_val = model_err.result().numpy()\n",
    "    test_rmse.append(rmse_val)\n",
    "    \n",
    "    # MAE 계산\n",
    "    model_err = metrics.MeanAbsoluteError()\n",
    "    model_err.update_state(y_test, y_pred)\n",
    "    mae_val = model_err.result().numpy()\n",
    "    test_mae.append(mae_val)\n",
    "    \n",
    "    \n",
    "    # acc 계산\n",
    "    #acc_val = np.mean((y_pred*10>=5)==y_test_bin)\n",
    "    #test_acc.append(acc_val)\n",
    "    \n",
    "    # rename\n",
    "    os.rename(odir, f'{rootdir}/mae{mae_val:.3f}_rmse{rmse_val:.3f}_{odir_f}')\n",
    "\n",
    "    # train 과정에서의 err\n",
    "    train_err = min(hist.history['loss'])\n",
    "    val_err = min(hist.history['val_loss'])\n",
    "\n",
    "    val_errs.append(val_err)\n",
    "    train_errs.append(train_err)\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "max_idx = test_roc.index(max(test_auc))\n",
    "print('\\nBest Model mae:{:.4f}, info: {}'.format(test_mae(max_idx), random_settings(max_idx)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c3d46-d870-4d7a-b41e-261f01d12e42",
   "metadata": {},
   "source": [
    "## Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966cf68f-6c01-4cd5-a424-3e8564c04378",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "        # model 학습 설정\n",
    "        try:\n",
    "            model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=[\"acc\", tf.keras.metrics.AUC()])\n",
    "            hist = model.fit(x_train_ecg, y_train_bin, sample_weight=train_w_samp3, validation_data=(x_val_ecg, y_val_bin, val_w_samp3), epochs=100, batch_size=batch_size, #class_weight={0:1, 1:3}, \n",
    "                                    callbacks=[ModelCheckpoint(monitor='val_loss', filepath=weightcache, verbose=1, save_best_only=True),\n",
    "                                                EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')])\n",
    "        except:\n",
    "            os.rmdir(odir)\n",
    "            #os.rename(odir,rootdir+'/error_{}'.format(odir_f))\n",
    "            itrial -= 1\n",
    "            test_roc.append(0)\n",
    "            test_acc.append(0)\n",
    "            test_prc.append(0)\n",
    "            train_errs.append(-1)\n",
    "            val_errs.append(-1)\n",
    "            continue\n",
    "            \n",
    "            \n",
    "    # 모델의 아키텍처 및 구조 저장\n",
    "    open(odir+\"/model.json\", \"wt\").write(model.to_json())\n",
    "\n",
    "    # test set에 대한 y_pred 계산\n",
    "    model.load_weights(weightcache)  # fit 함수는 마지막 epoch의 결과를 리턴하기 때문에 best 결과를 다시 읽어들어야함\n",
    "    y_pred = model.predict(x_test_ecg).flatten()\n",
    "\n",
    "    \n",
    "    # acc 계산\n",
    "    acc = metrics.Accuracy()\n",
    "    acc.update_state(y_pred>=0.5, y_test_bin, sample_weight=test_w_samp3)\n",
    "    acc_val = acc.result().numpy()\n",
    "    test_acc.append(acc_val)\n",
    "    \n",
    "    # auroc 계산\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test_bin, y_pred, sample_weight=test_w_samp3)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_roc.append(roc_auc)\n",
    "\n",
    "    # auprc \n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin, y_pred, sample_weight=test_w_samp3)\n",
    "    prc_auc = auc(recall, precision)\n",
    "    test_prc.append(prc_auc)\n",
    "\n",
    "    \n",
    "    # rename\n",
    "    os.rename(odir, rootdir+'/roc{:.4f}_prc{:.4f}_{}_acc{:.2f}'.format(roc_auc, prc_auc, odir_f, acc_val))\n",
    "\n",
    "    # train 과정에서의 err\n",
    "    train_err = min(hist.history['loss'])\n",
    "    val_err = min(hist.history['val_loss'])\n",
    "\n",
    "    val_errs.append(val_err)\n",
    "    train_errs.append(train_err)\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "max_idx = test_roc.index(max(test_auc))\n",
    "print('\\nBest Model roc:{:.4f}, info: {}'.format(test_roc(max_idx), random_settings(max_idx)))\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "painstudy_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
